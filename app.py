import streamlit as st
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from pyvis.network import Network
import community as community_louvain 
import numpy as np
from collections import Counter, defaultdict
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
import os
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="RedCiencia Cuba",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("RedCiencia Cuba")


@st.cache_data
def load_and_analyze_data():
    """Carga y analiza los datos de papers cubanos"""
    try:
        df = pd.read_csv('data_final_normalizado.csv')
        
        # Limpieza y preprocesamiento
        df['autores_normalizados'] = df['autores_normalizados'].fillna('').astype(str)
        df['afiliaciones_normalizadas'] = df['afiliaciones_normalizadas'].fillna('').astype(str)
        df['tematica'] = df['tematica'].fillna('Sin tem√°tica').astype(str)
        df['palabras_clave'] = df['palabras_clave'].fillna('').astype(str)
        df['titulo'] = df['titulo'].fillna('').astype(str)
        
        # Procesamiento de listas
        df['autores_list'] = df['autores_normalizados'].apply(
            lambda x: [a.strip() for a in x.split(',') if a.strip()]
        )
        df['afiliaciones_list'] = df['afiliaciones_normalizadas'].apply(
            lambda x: [i.strip() for i in x.split(',') if i.strip()]
        )
        df['palabras_clave_list'] = df['palabras_clave'].apply(
            lambda x: [k.strip() for k in x.split('|') if k.strip()]
        )
        
        # Filtrar papers sin autores
        df = df[df['autores_list'].apply(len) > 0]
        
        # An√°lisis exploratorio
        stats = {
            'total_papers': len(df),
            'total_autores': len(set([autor for autores_list in df['autores_list'] for autor in autores_list])),
            'total_instituciones': len(set([inst for afil_list in df['afiliaciones_list'] for inst in afil_list if inst])),
            'total_palabras_clave': len(set([kw for kw_list in df['palabras_clave_list'] for kw in kw_list if kw])),
        }
        
        return df, stats
    except Exception as e:
        st.error(f"Error cargando datos: {e}")
        return None, None

def create_sidebar_filters(df):
    """Crea filtros funcionales en el sidebar"""
    
    st.sidebar.header("Filtros para Subgrafos")
    
    tematicas_disponibles = ['Todas'] + sorted([t for t in df['tematica'].unique() if t != 'Sin tem√°tica'])
    tematica_seleccionada = st.sidebar.selectbox(
        "üìö Filtrar por Tem√°tica:",
        tematicas_disponibles,
        help="Selecciona una tem√°tica espec√≠fica para analizar"
    )
    
    todas_instituciones = set([inst for afil_list in df['afiliaciones_list'] for inst in afil_list if inst])
    # Filtrar instituciones que aparecen al menos 2 veces
    inst_counts = Counter([inst for afil_list in df['afiliaciones_list'] for inst in afil_list if inst])
    instituciones_frecuentes = [inst for inst, count in inst_counts.items() if count >= 2]
    instituciones_principales = ['Todas'] + sorted(instituciones_frecuentes)[:20]
    
    institucion_seleccionada = st.sidebar.selectbox(
        "üè´ Filtrar por Instituci√≥n:",
        instituciones_principales,
        help="Selecciona una instituci√≥n espec√≠fica"
    )
    
    # Filtro por colaboraci√≥n m√≠nima
    min_colaboraciones = st.sidebar.slider(
        "ü§ù Colaboraciones M√≠nimas:",
        min_value=1,
        max_value=5,
        value=1,
        help="N√∫mero m√≠nimo de colaboraciones entre autores"
    )
    
    # Filtro por tama√±o de red
    max_nodos_red = st.sidebar.slider(
        "M√°ximo Nodos en Visualizaci√≥n:",
        min_value=50,
        max_value=500,
        value=200,
        help="Limita el tama√±o de la red para mejor rendimiento"
    )
    
    return {
        'tematica': tematica_seleccionada,
        'institucion': institucion_seleccionada,
        'min_colaboraciones': min_colaboraciones,
        'max_nodos': max_nodos_red
    }

def apply_filters(df, filters):
    """Aplica los filtros seleccionados al dataframe"""
    df_filtered = df.copy()
    
    if filters['tematica'] != 'Todas':
        df_filtered = df_filtered[df_filtered['tematica'] == filters['tematica']]
    
    if filters['institucion'] != 'Todas':
        mask = df_filtered['afiliaciones_list'].apply(
            lambda x: filters['institucion'] in x
        )
        df_filtered = df_filtered[mask]
    
    return df_filtered

@st.cache_data
def build_filtered_coauthorship_network(df_filtered, min_collaborations=1, max_nodes=200):
    """Construye red de coautor√≠a con filtros aplicados"""
    G = nx.Graph()
    edge_weights = defaultdict(int)
    
    # Contar colaboraciones
    for autores_list in df_filtered['autores_list']:
        if len(autores_list) < 2:
            continue
        for i in range(len(autores_list)):
            for j in range(i + 1, len(autores_list)):
                edge = tuple(sorted([autores_list[i], autores_list[j]]))
                edge_weights[edge] += 1
    
    # Construir grafo con filtro de colaboraciones m√≠nimas
    for (autor1, autor2), weight in edge_weights.items():
        if weight >= min_collaborations:
            G.add_edge(autor1, autor2, weight=weight)
    
    # Limitar nodos si es necesario
    if G.number_of_nodes() > max_nodes:
        # Seleccionar nodos m√°s conectados
        top_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:max_nodes]
        nodes_to_keep = [node for node, degree in top_nodes]
        G = G.subgraph(nodes_to_keep).copy()
    
    return G

@st.cache_data
def build_filtered_institution_network(df_filtered, min_shared_authors=1, max_nodes=100):
    """Construye red de colaboraci√≥n institucional filtrada"""
    # Primero construir red bipartita
    B = nx.Graph()
    
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        instituciones = row['afiliaciones_list']
        
        if not autores or not instituciones:
            continue
            
        for autor in autores:
            B.add_node(autor, bipartite=0, type='autor')
        for inst in instituciones:
            B.add_node(inst, bipartite=1, type='institucion')
            for autor in autores:
                B.add_edge(autor, inst)
    
    # Proyectar a red de instituciones
    instituciones = {n for n, d in B.nodes(data=True) if d.get('bipartite') == 1}
    
    if len(instituciones) < 2:
        return nx.Graph()
    
    projected = nx.bipartite.weighted_projected_graph(B, instituciones)
    
    # Filtrar por autores compartidos m√≠nimos
    if min_shared_authors > 1:
        edges_to_remove = [(u, v) for u, v, d in projected.edges(data=True) 
                            if d.get('weight', 1) < min_shared_authors]
        projected.remove_edges_from(edges_to_remove)
        projected.remove_nodes_from(list(nx.isolates(projected)))
    
    # Limitar nodos
    if projected.number_of_nodes() > max_nodes:
        top_nodes = sorted(projected.degree(), key=lambda x: x[1], reverse=True)[:max_nodes]
        nodes_to_keep = [node for node, degree in top_nodes]
        projected = projected.subgraph(nodes_to_keep).copy()
    
    return projected

@st.cache_data
def build_thematic_network(df_filtered, max_nodes=100):
    """Construye red bipartita Autor-Tem√°tica"""
    B = nx.Graph()
    
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        tematica = row['tematica'].strip()
        
        if not autores or not tematica or tematica == 'Sin tem√°tica':
            continue
            
        # Agregar nodo de tem√°tica si no existe
        if tematica not in B:
            B.add_node(tematica, bipartite=1, type='tematica')
        
        # Agregar autores y enlaces
        for autor in autores:
            if autor not in B:
                B.add_node(autor, bipartite=0, type='autor')
            B.add_edge(autor, tematica)
    
    # Limitar nodos si es necesario
    if B.number_of_nodes() > max_nodes:
        # Mantener tem√°ticas m√°s conectadas y sus autores
        tematicas = [n for n, d in B.nodes(data=True) if d.get('bipartite') == 1]
        tematica_degrees = [(t, B.degree(t)) for t in tematicas]
        top_tematicas = sorted(tematica_degrees, key=lambda x: x[1], reverse=True)[:10]
        
        nodes_to_keep = set([t for t, _ in top_tematicas])
        for tematica, _ in top_tematicas:
            neighbors = list(B.neighbors(tematica))
            nodes_to_keep.update(neighbors[:10])  # Top 10 autores por tem√°tica
        
        B = B.subgraph(nodes_to_keep).copy()
    
    return B

@st.cache_data
def build_keyword_network(df_filtered, max_nodes=150):
    """Construye red bipartita Autor-Palabra Clave"""
    B = nx.Graph()
    
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        palabras_clave = row['palabras_clave_list']
        
        if not autores or not palabras_clave:
            continue
            
        # Agregar autores
        for autor in autores:
            if autor not in B:
                B.add_node(autor, bipartite=0, type='autor')
        
        # Agregar palabras clave y enlaces
        for kw in palabras_clave:
            if kw not in B:
                B.add_node(kw, bipartite=1, type='palabra_clave')
            for autor in autores:
                B.add_edge(autor, kw)
    
    # Limitar nodos manteniendo palabras clave m√°s frecuentes
    if B.number_of_nodes() > max_nodes:
        keywords = [n for n, d in B.nodes(data=True) if d.get('bipartite') == 1]
        keyword_degrees = [(kw, B.degree(kw)) for kw in keywords]
        top_keywords = sorted(keyword_degrees, key=lambda x: x[1], reverse=True)[:20]
        
        nodes_to_keep = set([kw for kw, _ in top_keywords])
        for keyword, _ in top_keywords:
            neighbors = list(B.neighbors(keyword))
            nodes_to_keep.update(neighbors[:8])  # Top 8 autores por palabra clave
        
        B = B.subgraph(nodes_to_keep).copy()
    
    return B

#newww 
@st.cache_data
def build_institution_thematic_network(df_filtered, max_nodes=150):
    """Construye red bipartita Instituci√≥n-Tem√°tica"""
    B = nx.Graph()
    edge_weights = defaultdict(int)
    
    # Contar las conexiones Instituci√≥n-Tem√°tica
    for _, row in df_filtered.iterrows():
        instituciones = row['afiliaciones_list']
        tematica = row['tematica'].strip()
        
        if not instituciones or not tematica or tematica == 'Sin tem√°tica':
            continue
            
        for inst in set(instituciones): # Usamos set() para no contar doble la misma instituci√≥n en un solo paper
            edge_weights[(inst, tematica)] += 1
            
    # Construir el grafo con los pesos calculados
    for (inst, tematica), weight in edge_weights.items():
        # Agregar nodos con sus atributos
        if inst not in B:
            B.add_node(inst, bipartite=0, type='institucion')
        if tematica not in B:
            B.add_node(tematica, bipartite=1, type='tematica')
        
        # Agregar el enlace con su peso
        B.add_edge(inst, tematica, weight=weight)
        
    # Limitar nodos si es necesario (opcional pero recomendado)
    if B.number_of_nodes() > max_nodes:
        # Priorizar nodos con mayor peso total de sus conexiones
        node_weights = {node: B.degree(node, weight='weight') for node in B.nodes()}
        top_nodes_sorted = sorted(node_weights.items(), key=lambda item: item[1], reverse=True)
        
        nodes_to_keep = [node for node, weight in top_nodes_sorted[:max_nodes]]
        B = B.subgraph(nodes_to_keep).copy()
        # Eliminar nodos que quedaron aislados despu√©s del subgrafo
        B.remove_nodes_from(list(nx.isolates(B)))
        
    return B


def calculate_network_metrics(G, network_type="general"):
    """Calcula m√©tricas espec√≠ficas para cada red"""
    if not G.nodes():
        return {}
    
    metrics = {}
    
    # M√©tricas b√°sicas
    metrics['num_nodes'] = G.number_of_nodes()
    metrics['num_edges'] = G.number_of_edges()
    metrics['density'] = nx.density(G)
    
    # M√©tricas avanzadas solo para redes no muy grandes
    if G.number_of_nodes() <= 300:
        metrics['avg_clustering'] = nx.average_clustering(G)
        
        # Componentes conectados
        components = list(nx.connected_components(G))
        metrics['num_components'] = len(components)
        metrics['largest_component_size'] = len(max(components, key=len)) if components else 0
        
        # Centralidades
        metrics['degree_centrality'] = nx.degree_centrality(G)
        metrics['betweenness_centrality'] = nx.betweenness_centrality(G, k=min(100, G.number_of_nodes()))
        metrics['closeness_centrality'] = nx.closeness_centrality(G)
        
        # Eigenvector centrality para componente m√°s grande
        if nx.is_connected(G):
            try:
                metrics['eigenvector_centrality'] = nx.eigenvector_centrality(G, max_iter=500)
            except:
                metrics['eigenvector_centrality'] = {}
        else:
            largest_cc = max(nx.connected_components(G), key=len)
            if len(largest_cc) > 1:
                subgraph = G.subgraph(largest_cc)
                try:
                    metrics['eigenvector_centrality'] = nx.eigenvector_centrality(subgraph, max_iter=500)
                except:
                    metrics['eigenvector_centrality'] = {}
        
        # Detecci√≥n de comunidades
        if G.number_of_edges() > 0:
            try:
                partition = community_louvain.best_partition(G, random_state=42)
                metrics['communities'] = partition
                metrics['num_communities'] = len(set(partition.values()))
                metrics['modularity'] = community_louvain.modularity(partition, G)
            except:
                metrics['communities'] = {}
                metrics['num_communities'] = 0
                metrics['modularity'] = 0
    
    return metrics

def create_network_visualization(G, metrics, title="Red Cient√≠fica"):
    """Crea visualizaci√≥n interactiva de la red"""
    if not G.nodes():
        st.info("No hay nodos para visualizar con los filtros actuales.")
        return
    
    net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="#000000")
    net.toggle_physics(True)
    
    # Configuraci√≥n de f√≠sica optimizada
    net.set_options("""
    var options = {
        "physics": {
            "enabled": true,
            "stabilization": {"iterations": 100},
            "barnesHut": {
                "gravitationalConstant": -8000,
                "centralGravity": 0.3,
                "springLength": 95,
                "springConstant": 0.04,
                "damping": 0.09
            }
        }
    }
    """)
    
    # Colores para comunidades
    communities = metrics.get('communities', {})
    if communities:
        unique_communities = list(set(communities.values()))
        colors = px.colors.qualitative.Set3[:len(unique_communities)]
        color_map = {comm: colors[i % len(colors)] for i, comm in enumerate(unique_communities)}
    
    # Agregar nodos
    for node in G.nodes():
        # Tama√±o basado en grado
        degree = G.degree(node)
        size = 10 + min(np.log(degree + 1) * 5, 30)
        
        # Color basado en tipo de nodo o comunidad
        node_data = G.nodes[node] if hasattr(G, 'nodes') and node in G.nodes else {}
        
        if 'bipartite' in node_data:
            # Red bipartita
            if node_data['bipartite'] == 0:
                if node_data.get('type') == 'institucion':
                    color = "#45B7D1"  # Azul para instituciones
                    label = f"{node[:25]}..." if len(node) > 25 else f"{node}"
                else:
                    color = "#FF6B6B"  # Rojo para autores
                    label = f"{node[:15]}..." if len(node) > 15 else f"{node}"
            else:
                if node_data.get('type') == 'tematica':
                    color = "#52CD4E"  # Verde  para tem√°ticas
                    label = f"{node[:20]}..." if len(node) > 20 else f"{node}"
                else:
                    color = "#45B7D1"  # Azul para palabras clave
                    label = f"{node[:15]}..." if len(node) > 15 else f"{node}"
        else:
            # Red proyectada
            if communities and node in communities:
                color = color_map.get(communities[node], "#97C2FC")
                label = node[:20] + "..." if len(node) > 20 else node
            else:
                color = "#97C2FC"
                label = node[:20] + "..." if len(node) > 20 else node
        
        title = f"{node}<br>Grado: {degree}"
        if communities and node in communities:
            title += f"<br>Comunidad: {communities[node]}"
        
        net.add_node(node, label=label, size=size, color=color, title=title)
    
    # Agregar enlaces
    for source, target, data in G.edges(data=True):
        weight = data.get('weight', 1)
        width = min(0.5 + np.log(weight + 1) * 0.8, 8)
        title = f"Peso: {weight}" if weight > 1 else "Conexi√≥n"
        net.add_edge(source, target, width=width, title=title)
    
    # Guardar y mostrar
    try:
        path = "temp_network.html"
        net.save_graph(path)
        with open(path, 'r', encoding='utf-8') as f:
            html = f.read()
        st.components.v1.html(html, height=600, scrolling=True)
        
        # Limpiar archivo temporal
        if os.path.exists(path):
            os.remove(path)
    except Exception as e:
        st.error(f"Error en visualizaci√≥n: {e}")

def create_metrics_dashboard(metrics):
    """Crea dashboard de m√©tricas de la red"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Nodos", metrics.get('num_nodes', 0))
        if 'density' in metrics:
            st.metric("Densidad", f"{metrics['density']:.3f}")
    
    with col2:
        st.metric("Enlaces", metrics.get('num_edges', 0))
        if 'avg_clustering' in metrics:
            st.metric("Clustering Promedio", f"{metrics['avg_clustering']:.3f}")
    
    with col3:
        if 'num_components' in metrics:
            st.metric("Componentes", metrics['num_components'])
        if 'num_communities' in metrics:
            st.metric("Comunidades", metrics['num_communities'])
    
    with col4:
        if 'largest_component_size' in metrics:
            st.metric("Componente Mayor", metrics['largest_component_size'])
        if 'modularity' in metrics:
            st.metric("Modularidad", f"{metrics['modularity']:.3f}")

def create_single_centrality_plot(metrics, centrality_type, top_n):
    """Crea un solo gr√°fico de centralidad seg√∫n la selecci√≥n"""
    centrality_map = {
        'Grado': 'degree_centrality',
        'Intermediaci√≥n': 'betweenness_centrality',
        'Cercan√≠a': 'closeness_centrality',
        'Autovector': 'eigenvector_centrality'
    }
    
    metric_key = centrality_map.get(centrality_type)
    
    if not metric_key or metric_key not in metrics or not metrics[metric_key]:
        st.info(f"M√©trica de {centrality_type} no disponible para esta red.")
        return
    
    # Obtener datos y ordenar
    centrality_data = metrics[metric_key]
    sorted_data = sorted(centrality_data.items(), key=lambda x: x[1], reverse=True)[:top_n]
    
    if not sorted_data:
        st.info(f"No hay datos de centralidad de {centrality_type}.")
        return
    
    # Crear gr√°fico
    fig = go.Figure(go.Bar(
        x=[item[1] for item in sorted_data],
        y=[item[0] for item in sorted_data],
        orientation='h',
        marker_color='lightblue'
    ))
    
    fig.update_layout(
        title=f"Top {top_n} - Centralidad de {centrality_type}",
        xaxis_title="Valor de Centralidad",
        yaxis_title="Nodo",
        height=400 + top_n * 15,  # Altura din√°mica
        showlegend=False
    )
    
    st.plotly_chart(fig, use_container_width=True)

def analyze_author_collaboration_patterns(df_filtered, G_coautor, metrics_coautor):
    """An√°lisis espec√≠fico de patrones de colaboraci√≥n entre autores"""
    # Crear an√°lisis de autores m√°s colaborativos
    autor_stats = {}
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        if len(autores) > 1:  # Solo papers colaborativos
            for autor in autores:
                if autor not in autor_stats:
                    autor_stats[autor] = {
                        'colaboraciones': 0,
                        'coautores_unicos': set(),
                        'tematicas': set(),
                        'palabras_clave': [],
                        'instituciones': set()
                    }
                
                autor_stats[autor]['colaboraciones'] += 1
                autor_stats[autor]['coautores_unicos'].update([a for a in autores if a != autor])
                autor_stats[autor]['tematicas'].add(row['tematica'])
                autor_stats[autor]['palabras_clave'].extend(row['palabras_clave_list'])
                autor_stats[autor]['instituciones'].update(row['afiliaciones_list'])
    
    if autor_stats:
        # Convertir a DataFrame para an√°lisis
        autor_df = []
        for autor, stats in autor_stats.items():
            autor_df.append({
                'Autor': autor,
                'Colaboraciones': stats['colaboraciones'],
                'Coautores_Unicos': len(stats['coautores_unicos']),
                'Diversidad_Tematica': len(stats['tematicas']),
                'Diversidad_Institucional': len([inst for inst in stats['instituciones'] if inst]),
                'Palabras_Clave': stats['palabras_clave'],
                'Tematicas': list(stats['tematicas'])
            })
        
        if autor_df:  # Verificar que hay datos
            autor_df = pd.DataFrame(autor_df)
            autor_df = autor_df.sort_values('Colaboraciones', ascending=False)
            
        else:
            st.warning("No hay suficientes datos de colaboraci√≥n para realizar el an√°lisis.")
            return
        
        top_colaborativos = autor_df.head(10)
        
        fig = px.bar(
            top_colaborativos,
            x='Colaboraciones',
            y='Autor',
            orientation='h',
            title="Autores con mayor cantidad de Colaboraciones"
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
    
        # An√°lisis detallado de autor seleccionado
        st.subheader("Resumen de Autor")
        autor_seleccionado = st.selectbox(
            "Selecciona un autor:",
            [''] + list(autor_df['Autor'].values),
            key='autor_colab_analysis'
        )
        
        if autor_seleccionado:
            autor_info = autor_df[autor_df['Autor'] == autor_seleccionado].iloc[0]
            
            col1, col2, col3 = st.columns(3)
            with col1:
                st.metric("ü§ù Colaboraciones", autor_info['Colaboraciones'])
            with col2:
                st.metric("üìö Tem√°ticas", autor_info['Diversidad_Tematica'])
            with col3:
                st.metric("üèõÔ∏è Instituciones", autor_info['Diversidad_Institucional'])
            
            # Nube de palabras de especializaci√≥n
            if autor_info['Palabras_Clave']:
                st.subheader(f"‚òÅÔ∏è Campo de investigaci√≥n de {autor_seleccionado}")
                
                palabra_freq = Counter(autor_info['Palabras_Clave'])
                if len(palabra_freq) > 0:
                    try:
                        wordcloud = WordCloud(
                            width=800, 
                            height=300, 
                            background_color='white',
                            max_words=30,
                            colormap='viridis'
                        ).generate_from_frequencies(palabra_freq)
                        
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        plt.close()
                    except:
                        st.write("**Principales √°reas de especializaci√≥n:**")
                        for palabra, freq in palabra_freq.most_common(15):
                            st.write(f"‚Ä¢ {palabra} ({freq} veces)")
            
            # Tem√°ticas del autor
            col1, col2 = st.columns(2)
            with col1:
                st.write("**Tem√°ticas de Investigaci√≥n:**")
                for tema in autor_info['Tematicas']:
                    st.write(f"‚Ä¢ {tema}")

def analyze_institutional_collaboration(df_filtered, G_institucional, metrics_inst):
    """An√°lisis espec√≠fico de colaboraci√≥n institucional"""
    
    # An√°lisis de instituciones
    inst_stats = {}
    for _, row in df_filtered.iterrows():
        instituciones = list(set(row['afiliaciones_list']))  # Eliminar duplicados
        instituciones = [inst for inst in instituciones if inst]  # Filtrar vac√≠os
        
        for inst in instituciones:
            if inst not in inst_stats:
                inst_stats[inst] = {
                    'papers': 0,
                    'autores': set(),
                    'tematicas': set(),
                    'palabras_clave': [],
                    'colaboraciones_inst': set()
                }
            
            inst_stats[inst]['papers'] += 1
            inst_stats[inst]['autores'].update(row['autores_list'])
            inst_stats[inst]['tematicas'].add(row['tematica'])
            inst_stats[inst]['palabras_clave'].extend(row['palabras_clave_list'])
            
            # Colaboraciones con otras instituciones en el mismo paper
            otras_inst = [i for i in instituciones if i != inst]
            inst_stats[inst]['colaboraciones_inst'].update(otras_inst)
    
    if inst_stats:
        # Convertir a DataFrame
        inst_df = []
        for inst, stats in inst_stats.items():
            inst_df.append({
                'Instituci√≥n': inst,
                'Papers': stats['papers'],
                'Autores': len(stats['autores']),
                'Diversidad_Tematica': len(stats['tematicas']),
                'Colaboraciones_Inst': len(stats['colaboraciones_inst']),
                'Palabras_Clave': stats['palabras_clave'],
                'Tematicas': list(stats['tematicas']),
                'Especializaci√≥n': len(stats['tematicas']) / stats['papers'] if stats['papers'] > 0 else 0
            })
        
        if inst_df:  # Verificar que hay datos
            inst_df = pd.DataFrame(inst_df)
            inst_df = inst_df[inst_df['Papers'] >= 2]  # Filtrar instituciones con al menos 2 papers
            
            if len(inst_df) == 0:
                st.warning("No hay instituciones con suficientes papers para realizar el an√°lisis.")
                return
                
            inst_df = inst_df.sort_values('Papers', ascending=False)
            
        else:
            st.warning("No hay suficientes datos institucionales para realizar el an√°lisis.")
            return
        
        top_inst = inst_df.head(10)
        
        fig = px.bar(
            top_inst,
            x='Papers',
            y='Instituci√≥n',
            orientation='h',
            title="Instituciones con mayor cantidad de publicaciones de art√≠culos cientif√≠cos"
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        top_colab = inst_df.nlargest(10, 'Colaboraciones_Inst')
        
        fig = px.bar(
            top_colab,
            x='Colaboraciones_Inst',
            y='Instituci√≥n',
            orientation='h',
            title="Instituciones m√°s Colaboraciones"
        )
        fig.update_layout(height=400)
        st.plotly_chart(fig, use_container_width=True)
        
        # An√°lisis detallado de instituci√≥n
        st.subheader("Resumen de Instituci√≥n")
        inst_seleccionada = st.selectbox(
            "Selecciona una instituci√≥n:",
            [''] + list(inst_df['Instituci√≥n'].values),
            key='inst_analysis'
        )
        
        if inst_seleccionada:
            inst_info = inst_df[inst_df['Instituci√≥n'] == inst_seleccionada].iloc[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìÑ Papers", inst_info['Papers'])
            with col2:
                st.metric("üë• Autores", inst_info['Autores'])
            with col3:
                st.metric("üìö Tem√°ticas", inst_info['Diversidad_Tematica'])
            with col4:
                st.metric("ü§ù Colaboraciones", inst_info['Colaboraciones_Inst'])
            
            # Nube de palabras de especializaci√≥n institucional
            if inst_info['Palabras_Clave']:
                st.subheader(f"‚òÅÔ∏è √Åreas de Especializaci√≥n - {inst_seleccionada}")
                
                palabra_freq = Counter(inst_info['Palabras_Clave'])
                if len(palabra_freq) > 0:
                    try:
                        wordcloud = WordCloud(
                            width=800, 
                            height=300, 
                            background_color='white',
                            max_words=40,
                            colormap='plasma'
                        ).generate_from_frequencies(palabra_freq)
                        
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        plt.close()
                    except:
                        st.write("**Principales √°reas de investigaci√≥n:**")
                        for palabra, freq in palabra_freq.most_common(20):
                            st.write(f"‚Ä¢ {palabra} ({freq} veces)")
        
            with col1:
                st.write("**Tem√°ticas de Investigaci√≥n:**")
                for tema in inst_info['Tematicas'][:10]:
                    st.write(f"‚Ä¢ {tema}")
            
            
def analyze_thematic_patterns(df_filtered, G_tematica, metrics_tema):
    """An√°lisis espec√≠fico de patrones tem√°ticos"""
    
    # An√°lisis de tem√°ticas
    tema_stats = {}
    for _, row in df_filtered.iterrows():
        tematica = row['tematica']
        if tematica not in tema_stats:
            tema_stats[tematica] = {
                'papers': 0,
                'autores': set(),
                'instituciones': set(),
                'palabras_clave': [],
                'colaboraciones': 0
            }
        
        tema_stats[tematica]['papers'] += 1
        tema_stats[tematica]['autores'].update(row['autores_list'])
        tema_stats[tematica]['instituciones'].update([inst for inst in row['afiliaciones_list'] if inst])
        tema_stats[tematica]['palabras_clave'].extend(row['palabras_clave_list'])
        
        # Contar colaboraciones (papers con m√°s de un autor)
        if len(row['autores_list']) > 1:
            tema_stats[tematica]['colaboraciones'] += 1
    
    if tema_stats:
        # Convertir a DataFrame
        tema_df = []
        for tema, stats in tema_stats.items():
            tema_df.append({
                'Tem√°tica': tema,
                'Papers': stats['papers'],
                'Autores': len(stats['autores']),
                'Instituciones': len(stats['instituciones']),
                'Colaboraciones': stats['colaboraciones'],
                'Tasa_Colaboracion': stats['colaboraciones'] / stats['papers'] if stats['papers'] > 0 else 0,
                'Palabras_Clave': stats['palabras_clave']
            })
        
        if tema_df:  # Verificar que hay datos
            tema_df = pd.DataFrame(tema_df)
            tema_df = tema_df[tema_df['Papers'] >= 2]
            
            if len(tema_df) == 0:
                st.warning("No hay tem√°ticas con suficientes papers para realizar el an√°lisis.")
                return
                
            tema_df = tema_df.sort_values('Papers', ascending=False)
            
            col1, col2 = st.columns(2)
        else:
            st.warning("No hay suficientes datos tem√°ticos para realizar el an√°lisis.")
            return
        
        st.subheader("Resumen de Tem√°tica")
        tema_seleccionada = st.selectbox(
            "Selecciona una tem√°tica para an√°lisis detallado:",
            [''] + list(tema_df['Tem√°tica'].values),
            key='tema_analysis'
        )
        
        if tema_seleccionada:
            tema_info = tema_df[tema_df['Tem√°tica'] == tema_seleccionada].iloc[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìÑ Papers", tema_info['Papers'])
            with col2:
                st.metric("üë• Autores", tema_info['Autores'])
            with col3:
                st.metric("üèõÔ∏è Instituciones", tema_info['Instituciones'])
            with col4:
                st.metric("ü§ù Tasa Colaboraci√≥n", f"{tema_info['Tasa_Colaboracion']:.2%}")
            
            # Nube de palabras de la tem√°tica
            if tema_info['Palabras_Clave']:
                st.subheader(f"‚òÅÔ∏è Palabras Clave - {tema_seleccionada}")
                
                palabra_freq = Counter(tema_info['Palabras_Clave'])
                if len(palabra_freq) > 0:
                    try:
                        wordcloud = WordCloud(
                            width=800, 
                            height=300, 
                            background_color='white',
                            max_words=50,
                            colormap='coolwarm'
                        ).generate_from_frequencies(palabra_freq)
                        
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        plt.close()
                    except:
                        st.write("**Principales palabras clave:**")
                        for palabra, freq in palabra_freq.most_common(25):
                            st.write(f"‚Ä¢ {palabra} ({freq} veces)")

def analyze_keyword_patterns(df_filtered, G_keywords, metrics_kw):
    """An√°lisis espec√≠fico de patrones de palabras clave"""
    st.subheader("üîç An√°lisis de Datos: Patrones de Palabras Clave")
    
    # An√°lisis de palabras clave m√°s frecuentes
    all_keywords = []
    keyword_authors = {}
    keyword_tematicas = {}
    
    for _, row in df_filtered.iterrows():
        palabras_clave = row['palabras_clave_list']
        autores = row['autores_list']
        tematica = row['tematica']
        
        for kw in palabras_clave:
            if kw:
                all_keywords.append(kw)
                
                if kw not in keyword_authors:
                    keyword_authors[kw] = set()
                    keyword_tematicas[kw] = set()
                
                keyword_authors[kw].update(autores)
                keyword_tematicas[kw].add(tematica)
    
    if all_keywords:
        # An√°lisis de frecuencia de palabras clave
        keyword_freq = Counter(all_keywords)
        
        # Crear DataFrame para an√°lisis
        kw_df = []
        for kw, freq in keyword_freq.items():
            if freq >= 2:  # Solo palabras clave que aparecen al menos 2 veces
                kw_df.append({
                    'Palabra_Clave': kw,
                    'Frecuencia': freq,
                    'Num_Autores': len(keyword_authors[kw]),
                    'Num_Tematicas': len(keyword_tematicas[kw]),
                    'Autores': list(keyword_authors[kw]),
                    'Tematicas': list(keyword_tematicas[kw])
                })
        
        if kw_df:  # Verificar que hay datos
            kw_df = pd.DataFrame(kw_df)
            kw_df = kw_df.sort_values('Frecuencia', ascending=False)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**üèÜ Palabras Clave M√°s Frecuentes**")
                top_keywords = kw_df.head(15)
                
                fig = px.bar(
                    top_keywords,
                    x='Frecuencia',
                    y='Palabra_Clave',
                    orientation='h',
                    title="Palabras Clave M√°s Utilizadas"
                )
                fig.update_layout(height=500)
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.write("**üåê Diversidad de Palabras Clave**")
                
                fig_scatter = px.scatter(
                    kw_df.head(20),
                    x='Frecuencia',
                    y='Num_Autores',
                    size='Num_Tematicas',
                    hover_name='Palabra_Clave',
                    title="Frecuencia vs N√∫mero de Autores",
                    labels={
                        'Frecuencia': 'Frecuencia de Uso',
                        'Num_Autores': 'N√∫mero de Autores',
                        'Num_Tematicas': 'N√∫mero de Tem√°ticas'
                    }
                )
                st.plotly_chart(fig_scatter, use_container_width=True)
            
            # An√°lisis de especializaci√≥n vs generalizaci√≥n
            st.subheader("üéØ Especializaci√≥n de Palabras Clave")
            
            # Clasificar palabras clave por especializaci√≥n
            kw_df['Especializaci√≥n'] = kw_df['Num_Tematicas'] / kw_df['Frecuencia']
            kw_df['Tipo_Palabra'] = kw_df['Especializaci√≥n'].apply(
                lambda x: 'Muy Espec√≠fica' if x <= 0.3 else 
                         'Espec√≠fica' if x <= 0.6 else 
                         'General'
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Distribuci√≥n de tipos de palabras
                tipo_counts = kw_df['Tipo_Palabra'].value_counts()
                
                fig_pie = px.pie(
                    values=tipo_counts.values,
                    names=tipo_counts.index,
                    title="Distribuci√≥n de Tipos de Palabras Clave"
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            with col2:
                st.write("**Palabras Clave M√°s Espec√≠ficas:**")
                especificas = kw_df[kw_df['Tipo_Palabra'] == 'Muy Espec√≠fica'].head(10)
                for _, row in especificas.iterrows():
                    st.write(f"‚Ä¢ **{row['Palabra_Clave']}** ({row['Frecuencia']} usos, {row['Num_Tematicas']} tem√°ticas)")
            
            # An√°lisis detallado de palabra clave
            st.subheader("üîç An√°lisis Detallado de Palabra Clave")
            kw_seleccionada = st.selectbox(
                "Selecciona una palabra clave para an√°lisis detallado:",
                [''] + list(kw_df['Palabra_Clave'].values),
                key='kw_analysis'
            )
            
            if kw_seleccionada:
                kw_info = kw_df[kw_df['Palabra_Clave'] == kw_seleccionada].iloc[0]
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üìä Frecuencia", kw_info['Frecuencia'])
                with col2:
                    st.metric("üë• Autores", kw_info['Num_Autores'])
                with col3:
                    st.metric("üìö Tem√°ticas", kw_info['Num_Tematicas'])
                with col4:
                    tipo = kw_info['Tipo_Palabra']
                    if tipo == 'Muy Espec√≠fica':
                        st.success("üéØ Muy Espec√≠fica")
                    elif tipo == 'Espec√≠fica':
                        st.info("üìä Espec√≠fica")
                    else:
                        st.warning("üåê General")
                
                # Mostrar informaci√≥n detallada
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Principales Autores:**")
                    for autor in kw_info['Autores'][:10]:
                        st.write(f"‚Ä¢ {autor}")
                
                with col2:
                    st.write("**Tem√°ticas Relacionadas:**")
                    for tema in kw_info['Tematicas']:
                        st.write(f"‚Ä¢ {tema}")
        else:
            st.warning("No hay suficientes palabras clave que aparezcan m√∫ltiples veces para realizar el an√°lisis.")

def analyze_institution_thematic_patterns(df_filtered, G_inst_tema, metrics_inst_tema):
    """An√°lisis espec√≠fico de patrones instituci√≥n-tem√°tica"""

    
    # An√°lisis de especializaci√≥n institucional por tem√°tica
    inst_tema_stats = {}
    
    for _, row in df_filtered.iterrows():
        instituciones = [inst for inst in row['afiliaciones_list'] if inst]
        tematica = row['tematica']
        palabras_clave = row['palabras_clave_list']
        autores = row['autores_list']
        
        for inst in set(instituciones):
            if inst not in inst_tema_stats:
                inst_tema_stats[inst] = {}
            
            if tematica not in inst_tema_stats[inst]:
                inst_tema_stats[inst][tematica] = {
                    'papers': 0,
                    'autores': set(),
                    'palabras_clave': []
                }
            
            inst_tema_stats[inst][tematica]['papers'] += 1
            inst_tema_stats[inst][tematica]['autores'].update(autores)
            inst_tema_stats[inst][tematica]['palabras_clave'].extend(palabras_clave)
    
    if inst_tema_stats:
        # Crear an√°lisis de especializaci√≥n
        especializaci√≥n_data = []
        
        for inst, temas in inst_tema_stats.items():
            total_papers = sum(tema_data['papers'] for tema_data in temas.values())
            if total_papers >= 2:  # Solo instituciones con al menos 2 papers
                
                # Encontrar tem√°tica principal
                tema_principal = max(temas.items(), key=lambda x: x[1]['papers'])
                
                especializaci√≥n_data.append({
                    'Instituci√≥n': inst,
                    'Total_Papers': total_papers,
                    'Num_Tematicas': len(temas),
                    'Tema_Principal': tema_principal[0],
                    'Papers_Tema_Principal': tema_principal[1]['papers'],
                    'Porcentaje_Especializaci√≥n': (tema_principal[1]['papers'] / total_papers) * 100,
                    'Autores_Tema_Principal': len(tema_principal[1]['autores']),
                    'Palabras_Clave_Principal': tema_principal[1]['palabras_clave']
                })
        
        if especializaci√≥n_data:  # Verificar que hay datos
            esp_df = pd.DataFrame(especializaci√≥n_data)
            esp_df = esp_df.sort_values('Total_Papers', ascending=False)
            
            # Clasificar instituciones por nivel de especializaci√≥n
            esp_df['Nivel_Especializaci√≥n'] = esp_df['Porcentaje_Especializaci√≥n'].apply(
                lambda x: 'Muy Especializada' if x >= 70 else 
                            'Especializada' if x >= 50 else 
                            'Diversificada'
            )
        else:
            st.warning("No hay suficientes datos institucionales para realizar el an√°lisis de especializaci√≥n.")
            return
        
        
        # Agrupar por tem√°tica principal
        fortalezas = esp_df.groupby('Tema_Principal').agg({
            'Instituci√≥n': 'count',
            'Papers_Tema_Principal': 'sum',
            'Autores_Tema_Principal': 'sum'
        }).reset_index()
        
        fortalezas.columns = ['Tem√°tica', 'Num_Instituciones', 'Total_Papers', 'Total_Autores']
        fortalezas = fortalezas.sort_values('Total_Papers', ascending=False)
        
        
        # An√°lisis detallado de instituci√≥n
        st.subheader("Resumen de Especializaci√≥n Institucional")
        inst_seleccionada = st.selectbox(
            "Selecciona una instituci√≥n para an√°lisis detallado:",
            [''] + list(esp_df['Instituci√≥n'].values),
            key='inst_tema_analysis'
        )
        
        if inst_seleccionada:
            inst_info = esp_df[esp_df['Instituci√≥n'] == inst_seleccionada].iloc[0]
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("üìÑ Total Papers", inst_info['Total_Papers'])
            with col2:
                st.metric("üìö Tem√°ticas", inst_info['Num_Tematicas'])
            
            
            
            

##INTERFAZ PRINCIPAL
with st.spinner("üîÑ Cargando datos de papers cubanos..."):
    df, stats = load_and_analyze_data()

if df is None:
    st.stop()

filters = create_sidebar_filters(df)

df_filtered = apply_filters(df, filters)

tabs = st.tabs([
    "üìä General", 
    "ü§ù Red de Coautor√≠a", 
    "üè¢ Red Institucional", 
    "üìö Red Tem√°tica",
    "üèõÔ∏è Red Instituci√≥n-Tem√°tica"
])

with tabs[0]:
    st.subheader("üìä Estad√≠sticas Generales")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Papers", f"{stats['total_papers']:,}")
    with col2:
        st.metric("Total Autores", f"{stats['total_autores']:,}")
    with col3:
        st.metric("Total Instituciones", f"{stats['total_instituciones']:,}")
    with col4:
        st.metric("Total Palabras Clave", f"{stats['total_palabras_clave']:,}")
    
    st.subheader("üõ†Ô∏è Caracter√≠sticas de la Herramienta")

    st.markdown("""
    #### **Funcionalidades Principales:**
    - **Filtros**: Filtra por tem√°tica e instituci√≥n para crear subgrafos espec√≠ficos
    - **M√∫ltiples Tipos de Redes**: Coautor√≠a, institucional, tem√°tica, palabras clave, instituci√≥n-tem√°tica.
    - **Visualizaciones Interactivas**: Redes interactivas con informaci√≥n detallada al pasar el mouse
    - **M√©tricas de Centralidad**: Identifica autores e instituciones m√°s importantes
    - **Detecci√≥n de Comunidades**: Encuentra grupos de investigaci√≥n colaborativa
    """)
        
    st.info("üí°M√©tricas")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("""
        ##### **M√©tricas de Centralidad:**
        - **Centralidad de Grado**:  Qu√© tan conectado est√° un nodo. √ötil para identificar autores con muchas colaboraciones.
        - **Centralidad de Intermediaci√≥n (Betweenness)**: Qu√© tan frecuentemente un nodo est√° en el camino m√°s corto entre otros dos. Revela autores puente.
        - **Centralidad de Cercan√≠a (Closeness)**: Qu√© tan r√°pido puede acceder un nodo al resto de la red. Ideal para detectar nodos con buena difusi√≥n.
        - **Centralidad de Eigenvector:**: Eval√∫a la influencia de un nodo tomando en cuenta la importancia de sus vecinos
        """)
    
    with col2:
        st.markdown("""
        ##### **M√©tricas de agrupamiento y cohesi√≥n:**
        - **Coeficiente de agrupamiento (Clustering coefficient)**: Mide qu√© tan conectados est√°n los vecinos de un nodo entre s√≠. Perfecto para detectar comunidades cient√≠ficas.
        - **Modularidad**: Ayuda a identificar comunidades dentro de la red (ej. grupos de autores que suelen colaborar entre s√≠).
        """)
        
    with col3:
        st.markdown("""
        ##### **M√©tricas de alcance e influencia:**
        - **Densidad**: Cu√°ntos enlaces existen en relaci√≥n a todos los posibles. Una red muy densa podr√≠a indicar un campo de investigaci√≥n muy interconectado.
        """)
        
with tabs[1]:
    st.header("ü§ù Red de Coautor√≠a")
    st.markdown("""
    - Identifica patrones de colaboraci√≥n entre investigadores cubanos.
    - Revela qui√©nes son los autores m√°s conectados e influyentes.
    - Muestra comunidades de investigaci√≥n que trabajan juntas frecuentemente.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        with st.spinner("Construyendo red de coautor√≠a..."):
            G_coautor = build_filtered_coauthorship_network(
                df_filtered, 
                filters['min_colaboraciones'], 
                filters['max_nodos']
            )
        
        if G_coautor.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_coautor = calculate_network_metrics(G_coautor, "coautoria")
            
            create_network_visualization(G_coautor, metrics_coautor, "Red de Coautor√≠a")
            
            if 'degree_centrality' in metrics_coautor:
                st.subheader("M√©tricas de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona la m√©trica de centralidad deseada:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a', 'Eigenvector'],
                        key='centrality_coautor'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Cantidad a mostrar:",
                        [5, 10, 15, 20, 25],
                        index=1,
                        key='top_n_coautor'
                    )
                
                create_single_centrality_plot(metrics_coautor, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de colaboraci√≥n de autores
            analyze_author_collaboration_patterns(df_filtered, G_coautor, metrics_coautor)
        else:
            st.warning("No hay suficientes datos para construir la red de coautor√≠a con los filtros actuales.")

with tabs[2]:
    st.header("üè¢ Red de Colaboraci√≥n Institucional")
    st.markdown("""
    - Analiza colaboraciones entre instituciones cubanas.
    - Identifica instituciones que act√∫an como puentes entre diferentes grupos.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        min_shared = st.slider(
            "Autores compartidos m√≠nimos entre instituciones:", 
            1, 5, 1, 
            key='inst_shared',
            help="Dos instituciones se conectan si comparten al menos este n√∫mero de autores"
        )
        
        with st.spinner("Construyendo red institucional..."):
            G_institucional = build_filtered_institution_network(
                df_filtered, 
                min_shared, 
                filters['max_nodos']//2
            )
        
        if G_institucional.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_inst = calculate_network_metrics(G_institucional, "institucional")
            
            create_network_visualization(G_institucional, metrics_inst, "Red Institucional")
            
            if 'degree_centrality' in metrics_inst:
                st.write("M√©tricas de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona la m√©trica de centralidad deseada:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a', 'Eigenvector'],
                        key='centrality_inst'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Cantidad a mostrar:",
                        [5, 10, 15, 20],
                        index=1,
                        key='top_n_inst'
                    )
                
                create_single_centrality_plot(metrics_inst, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de colaboraci√≥n institucional
            analyze_institutional_collaboration(df_filtered, G_institucional, metrics_inst)
        else:
            st.warning("No hay suficientes datos para construir la red institucional con los filtros actuales.")

with tabs[3]:
    st.header("üìö Red Tem√°tica (Autor-Tem√°tica)")
    st.markdown("""
    - Conecta autores con las tem√°ticas que investigan.
    - Identifica especialistas en cada √°rea.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        with st.spinner("Construyendo red tem√°tica..."):
            G_tematica = build_thematic_network(df_filtered, filters['max_nodos'])
        
        if G_tematica.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_tema = calculate_network_metrics(G_tematica, "tematica")
            
            st.info("üî¥ Nodos rojos = Autores | üü¢ Nodos verdes = Tem√°ticas")
            create_network_visualization(G_tematica, metrics_tema, "Red Tem√°tica")
            
            # M√©tricas de centralidad para red bipartita
            if 'degree_centrality' in metrics_tema:
                st.subheader("M√©tricas de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona la m√©trica de centralidad deseada:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a'],
                        key='centrality_tema'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Cantidad a mostrar:",
                        [5, 10, 15, 20],
                        index=1,
                        key='top_n_tema'
                    )
                
                create_single_centrality_plot(metrics_tema, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de patrones tem√°ticos
            analyze_thematic_patterns(df_filtered, G_tematica, metrics_tema)
        else:
            st.warning("No hay suficientes datos para construir la red tem√°tica con los filtros actuales.")

with tabs[4]:
    st.header("üèõÔ∏è Red Instituci√≥n-Tem√°tica")
    st.markdown("""
    - Identifica fortalezas de investigaci√≥n, conecta instituciones con las tem√°ticas que publican.
    - Revela qu√© instituciones se especializan en qu√© √°reas.
    - Muestra instituciones con mayor diversidad de investigaci√≥n.
    - Identifica instituciones que comparten intereses de investigaci√≥n.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        with st.spinner("Construyendo red Instituci√≥n-Tem√°tica..."):
            G_inst_tema = build_institution_thematic_network(df_filtered, filters['max_nodos'])
        
        if G_inst_tema.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_inst_tema = calculate_network_metrics(G_inst_tema, "institucion_tematica")
            
            st.info("üîµ Nodos azules = Instituciones | üü¢ Nodos verdes = Tem√°ticas")
            create_network_visualization(G_inst_tema, metrics_inst_tema, "Red Instituci√≥n-Tem√°tica")
            
            # Crear an√°lisis de especializaci√≥n por instituci√≥n
            instituciones = [n for n, d in G_inst_tema.nodes(data=True) if d.get('bipartite') == 0]
            tematicas = [n for n, d in G_inst_tema.nodes(data=True) if d.get('bipartite') == 1]
            
            if instituciones and tematicas:
                # An√°lisis de diversidad tem√°tica por instituci√≥n
                inst_diversidad = {}
                inst_fortalezas = {}
                
                for inst in instituciones:
                    # Obtener tem√°ticas conectadas y sus pesos
                    temas_conectadas = []
                    pesos_temas = []
                    
                    for tema in G_inst_tema.neighbors(inst):
                        peso = G_inst_tema[inst][tema].get('weight', 1)
                        temas_conectadas.append(tema)
                        pesos_temas.append(peso)
                    
                    inst_diversidad[inst] = len(temas_conectadas)
                    
                    # Identificar fortaleza principal (tem√°tica con mayor peso)
                    if pesos_temas:
                        idx_max = pesos_temas.index(max(pesos_temas))
                        inst_fortalezas[inst] = {
                            'tema_principal': temas_conectadas[idx_max],
                            'num_papers': max(pesos_temas),
                            'diversidad': len(temas_conectadas),
                            'total_papers': sum(pesos_temas)
                        }
                
            # M√©tricas de centralidad
            if 'degree_centrality' in metrics_inst_tema:
                st.subheader("M√©tricas de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona m√©trica de centralidad:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a'],
                        key='centrality_inst_tema'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Cantidad a mostrar:",
                        [5, 10, 15, 20],
                        index=1,
                        key='top_n_inst_tema'
                    )

                create_single_centrality_plot(metrics_inst_tema, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de especializaci√≥n institucional
            analyze_institution_thematic_patterns(df_filtered, G_inst_tema, metrics_inst_tema)
        else:
            st.warning("No hay suficientes datos para construir la red Instituci√≥n-Tem√°tica con los filtros actuales.")
    