import streamlit as st
import pandas as pd
import networkx as nx
import matplotlib.pyplot as plt
from pyvis.network import Network
import community as community_louvain 
import numpy as np
from collections import Counter, defaultdict
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
import re
import os
import seaborn as sns
from wordcloud import WordCloud
import matplotlib.pyplot as plt
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
import warnings
warnings.filterwarnings('ignore')

st.set_page_config(
    page_title="RedCiencia Cuba",
    layout="wide",
    initial_sidebar_state="expanded"
)

st.title("RedCiencia Cuba")


@st.cache_data
def load_and_analyze_data():
    """Carga y analiza los datos de papers cubanos"""
    try:
        df = pd.read_csv('data_final_normalizado.csv')
        
        # Limpieza y preprocesamiento
        df['autores_normalizados'] = df['autores_normalizados'].fillna('').astype(str)
        df['afiliaciones_normalizadas'] = df['afiliaciones_normalizadas'].fillna('').astype(str)
        df['tematica'] = df['tematica'].fillna('Sin tem√°tica').astype(str)
        df['palabras_clave'] = df['palabras_clave'].fillna('').astype(str)
        df['titulo'] = df['titulo'].fillna('').astype(str)
        
        # Procesamiento de listas
        df['autores_list'] = df['autores_normalizados'].apply(
            lambda x: [a.strip() for a in x.split(',') if a.strip()]
        )
        df['afiliaciones_list'] = df['afiliaciones_normalizadas'].apply(
            lambda x: [i.strip() for i in x.split(',') if i.strip()]
        )
        df['palabras_clave_list'] = df['palabras_clave'].apply(
            lambda x: [k.strip() for k in x.split('|') if k.strip()]
        )
        
        # Filtrar papers sin autores
        df = df[df['autores_list'].apply(len) > 0]
        
        # An√°lisis exploratorio
        stats = {
            'total_papers': len(df),
            'total_autores': len(set([autor for autores_list in df['autores_list'] for autor in autores_list])),
            'total_instituciones': len(set([inst for afil_list in df['afiliaciones_list'] for inst in afil_list if inst])),
            'total_palabras_clave': len(set([kw for kw_list in df['palabras_clave_list'] for kw in kw_list if kw])),
        }
        
        return df, stats
    except Exception as e:
        st.error(f"Error cargando datos: {e}")
        return None, None

def create_sidebar_filters(df):
    """Crea filtros funcionales en el sidebar"""
    
    st.sidebar.header("Filtros para Subgrafos")
    
    tematicas_disponibles = ['Todas'] + sorted([t for t in df['tematica'].unique() if t != 'Sin tem√°tica'])
    tematica_seleccionada = st.sidebar.selectbox(
        "üìö Filtrar por Tem√°tica:",
        tematicas_disponibles,
        help="Selecciona una tem√°tica espec√≠fica para analizar"
    )
    
    todas_instituciones = set([inst for afil_list in df['afiliaciones_list'] for inst in afil_list if inst])
    # Filtrar instituciones que aparecen al menos 2 veces
    inst_counts = Counter([inst for afil_list in df['afiliaciones_list'] for inst in afil_list if inst])
    instituciones_frecuentes = [inst for inst, count in inst_counts.items() if count >= 2]
    instituciones_principales = ['Todas'] + sorted(instituciones_frecuentes)[:20]
    
    institucion_seleccionada = st.sidebar.selectbox(
        "üè´ Filtrar por Instituci√≥n:",
        instituciones_principales,
        help="Selecciona una instituci√≥n espec√≠fica"
    )
    
    # Filtro por colaboraci√≥n m√≠nima
    min_colaboraciones = st.sidebar.slider(
        "ü§ù Colaboraciones M√≠nimas:",
        min_value=1,
        max_value=5,
        value=1,
        help="N√∫mero m√≠nimo de colaboraciones entre autores"
    )
    
    # Filtro por tama√±o de red
    max_nodos_red = st.sidebar.slider(
        "M√°ximo Nodos en Visualizaci√≥n:",
        min_value=50,
        max_value=500,
        value=200,
        help="Limita el tama√±o de la red para mejor rendimiento"
    )
    
    return {
        'tematica': tematica_seleccionada,
        'institucion': institucion_seleccionada,
        'min_colaboraciones': min_colaboraciones,
        'max_nodos': max_nodos_red
    }

def apply_filters(df, filters):
    """Aplica los filtros seleccionados al dataframe"""
    df_filtered = df.copy()
    
    if filters['tematica'] != 'Todas':
        df_filtered = df_filtered[df_filtered['tematica'] == filters['tematica']]
    
    if filters['institucion'] != 'Todas':
        mask = df_filtered['afiliaciones_list'].apply(
            lambda x: filters['institucion'] in x
        )
        df_filtered = df_filtered[mask]
    
    return df_filtered

@st.cache_data
def build_filtered_coauthorship_network(df_filtered, min_collaborations=1, max_nodes=200):
    """Construye red de coautor√≠a con filtros aplicados"""
    G = nx.Graph()
    edge_weights = defaultdict(int)
    
    # Contar colaboraciones
    for autores_list in df_filtered['autores_list']:
        if len(autores_list) < 2:
            continue
        for i in range(len(autores_list)):
            for j in range(i + 1, len(autores_list)):
                edge = tuple(sorted([autores_list[i], autores_list[j]]))
                edge_weights[edge] += 1
    
    # Construir grafo con filtro de colaboraciones m√≠nimas
    for (autor1, autor2), weight in edge_weights.items():
        if weight >= min_collaborations:
            G.add_edge(autor1, autor2, weight=weight)
    
    # Limitar nodos si es necesario
    if G.number_of_nodes() > max_nodes:
        # Seleccionar nodos m√°s conectados
        top_nodes = sorted(G.degree(), key=lambda x: x[1], reverse=True)[:max_nodes]
        nodes_to_keep = [node for node, degree in top_nodes]
        G = G.subgraph(nodes_to_keep).copy()
    
    return G

@st.cache_data
def build_filtered_institution_network(df_filtered, min_shared_authors=1, max_nodes=100):
    """Construye red de colaboraci√≥n institucional filtrada"""
    # Primero construir red bipartita
    B = nx.Graph()
    
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        instituciones = row['afiliaciones_list']
        
        if not autores or not instituciones:
            continue
            
        for autor in autores:
            B.add_node(autor, bipartite=0, type='autor')
        for inst in instituciones:
            B.add_node(inst, bipartite=1, type='institucion')
            for autor in autores:
                B.add_edge(autor, inst)
    
    # Proyectar a red de instituciones
    instituciones = {n for n, d in B.nodes(data=True) if d.get('bipartite') == 1}
    
    if len(instituciones) < 2:
        return nx.Graph()
    
    projected = nx.bipartite.weighted_projected_graph(B, instituciones)
    
    # Filtrar por autores compartidos m√≠nimos
    if min_shared_authors > 1:
        edges_to_remove = [(u, v) for u, v, d in projected.edges(data=True) 
                            if d.get('weight', 1) < min_shared_authors]
        projected.remove_edges_from(edges_to_remove)
        projected.remove_nodes_from(list(nx.isolates(projected)))
    
    # Limitar nodos
    if projected.number_of_nodes() > max_nodes:
        top_nodes = sorted(projected.degree(), key=lambda x: x[1], reverse=True)[:max_nodes]
        nodes_to_keep = [node for node, degree in top_nodes]
        projected = projected.subgraph(nodes_to_keep).copy()
    
    return projected

@st.cache_data
def build_thematic_network(df_filtered, max_nodes=100):
    """Construye red bipartita Autor-Tem√°tica"""
    B = nx.Graph()
    
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        tematica = row['tematica'].strip()
        
        if not autores or not tematica or tematica == 'Sin tem√°tica':
            continue
            
        # Agregar nodo de tem√°tica si no existe
        if tematica not in B:
            B.add_node(tematica, bipartite=1, type='tematica')
        
        # Agregar autores y enlaces
        for autor in autores:
            if autor not in B:
                B.add_node(autor, bipartite=0, type='autor')
            B.add_edge(autor, tematica)
    
    # Limitar nodos si es necesario
    if B.number_of_nodes() > max_nodes:
        # Mantener tem√°ticas m√°s conectadas y sus autores
        tematicas = [n for n, d in B.nodes(data=True) if d.get('bipartite') == 1]
        tematica_degrees = [(t, B.degree(t)) for t in tematicas]
        top_tematicas = sorted(tematica_degrees, key=lambda x: x[1], reverse=True)[:10]
        
        nodes_to_keep = set([t for t, _ in top_tematicas])
        for tematica, _ in top_tematicas:
            neighbors = list(B.neighbors(tematica))
            nodes_to_keep.update(neighbors[:10])  # Top 10 autores por tem√°tica
        
        B = B.subgraph(nodes_to_keep).copy()
    
    return B

@st.cache_data
def build_keyword_network(df_filtered, max_nodes=150):
    """Construye red bipartita Autor-Palabra Clave"""
    B = nx.Graph()
    
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        palabras_clave = row['palabras_clave_list']
        
        if not autores or not palabras_clave:
            continue
            
        # Agregar autores
        for autor in autores:
            if autor not in B:
                B.add_node(autor, bipartite=0, type='autor')
        
        # Agregar palabras clave y enlaces
        for kw in palabras_clave:
            if kw not in B:
                B.add_node(kw, bipartite=1, type='palabra_clave')
            for autor in autores:
                B.add_edge(autor, kw)
    
    # Limitar nodos manteniendo palabras clave m√°s frecuentes
    if B.number_of_nodes() > max_nodes:
        keywords = [n for n, d in B.nodes(data=True) if d.get('bipartite') == 1]
        keyword_degrees = [(kw, B.degree(kw)) for kw in keywords]
        top_keywords = sorted(keyword_degrees, key=lambda x: x[1], reverse=True)[:20]
        
        nodes_to_keep = set([kw for kw, _ in top_keywords])
        for keyword, _ in top_keywords:
            neighbors = list(B.neighbors(keyword))
            nodes_to_keep.update(neighbors[:8])  # Top 8 autores por palabra clave
        
        B = B.subgraph(nodes_to_keep).copy()
    
    return B

#newww 
@st.cache_data
def build_institution_thematic_network(df_filtered, max_nodes=150):
    """Construye red bipartita Instituci√≥n-Tem√°tica"""
    B = nx.Graph()
    edge_weights = defaultdict(int)
    
    # Contar las conexiones Instituci√≥n-Tem√°tica
    for _, row in df_filtered.iterrows():
        instituciones = row['afiliaciones_list']
        tematica = row['tematica'].strip()
        
        if not instituciones or not tematica or tematica == 'Sin tem√°tica':
            continue
            
        for inst in set(instituciones): # Usamos set() para no contar doble la misma instituci√≥n en un solo paper
            edge_weights[(inst, tematica)] += 1
            
    # Construir el grafo con los pesos calculados
    for (inst, tematica), weight in edge_weights.items():
        # Agregar nodos con sus atributos
        if inst not in B:
            B.add_node(inst, bipartite=0, type='institucion')
        if tematica not in B:
            B.add_node(tematica, bipartite=1, type='tematica')
        
        # Agregar el enlace con su peso
        B.add_edge(inst, tematica, weight=weight)
        
    # Limitar nodos si es necesario (opcional pero recomendado)
    if B.number_of_nodes() > max_nodes:
        # Priorizar nodos con mayor peso total de sus conexiones
        node_weights = {node: B.degree(node, weight='weight') for node in B.nodes()}
        top_nodes_sorted = sorted(node_weights.items(), key=lambda item: item[1], reverse=True)
        
        nodes_to_keep = [node for node, weight in top_nodes_sorted[:max_nodes]]
        B = B.subgraph(nodes_to_keep).copy()
        # Eliminar nodos que quedaron aislados despu√©s del subgrafo
        B.remove_nodes_from(list(nx.isolates(B)))
        
    return B


def calculate_network_metrics(G, network_type="general"):
    """Calcula m√©tricas espec√≠ficas para cada red"""
    if not G.nodes():
        return {}
    
    metrics = {}
    
    # M√©tricas b√°sicas
    metrics['num_nodes'] = G.number_of_nodes()
    metrics['num_edges'] = G.number_of_edges()
    metrics['density'] = nx.density(G)
    
    # M√©tricas avanzadas solo para redes no muy grandes
    if G.number_of_nodes() <= 300:
        metrics['avg_clustering'] = nx.average_clustering(G)
        
        # Componentes conectados
        components = list(nx.connected_components(G))
        metrics['num_components'] = len(components)
        metrics['largest_component_size'] = len(max(components, key=len)) if components else 0
        
        # Centralidades
        metrics['degree_centrality'] = nx.degree_centrality(G)
        metrics['betweenness_centrality'] = nx.betweenness_centrality(G, k=min(100, G.number_of_nodes()))
        metrics['closeness_centrality'] = nx.closeness_centrality(G)
        
        # Eigenvector centrality para componente m√°s grande
        if nx.is_connected(G):
            try:
                metrics['eigenvector_centrality'] = nx.eigenvector_centrality(G, max_iter=500)
            except:
                metrics['eigenvector_centrality'] = {}
        else:
            largest_cc = max(nx.connected_components(G), key=len)
            if len(largest_cc) > 1:
                subgraph = G.subgraph(largest_cc)
                try:
                    metrics['eigenvector_centrality'] = nx.eigenvector_centrality(subgraph, max_iter=500)
                except:
                    metrics['eigenvector_centrality'] = {}
        
        # Detecci√≥n de comunidades
        if G.number_of_edges() > 0:
            try:
                partition = community_louvain.best_partition(G, random_state=42)
                metrics['communities'] = partition
                metrics['num_communities'] = len(set(partition.values()))
                metrics['modularity'] = community_louvain.modularity(partition, G)
            except:
                metrics['communities'] = {}
                metrics['num_communities'] = 0
                metrics['modularity'] = 0
    
    return metrics

def create_network_visualization(G, metrics, title="Red Cient√≠fica"):
    """Crea visualizaci√≥n interactiva de la red"""
    if not G.nodes():
        st.info("No hay nodos para visualizar con los filtros actuales.")
        return
    
    net = Network(height="600px", width="100%", bgcolor="#ffffff", font_color="#000000")
    net.toggle_physics(True)
    
    # Configuraci√≥n de f√≠sica optimizada
    net.set_options("""
    var options = {
        "physics": {
            "enabled": true,
            "stabilization": {"iterations": 100},
            "barnesHut": {
                "gravitationalConstant": -8000,
                "centralGravity": 0.3,
                "springLength": 95,
                "springConstant": 0.04,
                "damping": 0.09
            }
        }
    }
    """)
    
    # Colores para comunidades
    communities = metrics.get('communities', {})
    if communities:
        unique_communities = list(set(communities.values()))
        colors = px.colors.qualitative.Set3[:len(unique_communities)]
        color_map = {comm: colors[i % len(colors)] for i, comm in enumerate(unique_communities)}
    
    # Agregar nodos
    for node in G.nodes():
        # Tama√±o basado en grado
        degree = G.degree(node)
        size = 10 + min(np.log(degree + 1) * 5, 30)
        
        # Color basado en tipo de nodo o comunidad
        node_data = G.nodes[node] if hasattr(G, 'nodes') and node in G.nodes else {}
        
        if 'bipartite' in node_data:
            # Red bipartita
            if node_data['bipartite'] == 0:
                if node_data.get('type') == 'institucion':
                    color = "#45B7D1"  # Azul para instituciones
                    label = f"{node[:25]}..." if len(node) > 25 else f"{node}"
                else:
                    color = "#FF6B6B"  # Rojo para autores
                    label = f"{node[:15]}..." if len(node) > 15 else f"{node}"
            else:
                if node_data.get('type') == 'tematica':
                    color = "#52CD4E"  # Verde  para tem√°ticas
                    label = f"{node[:20]}..." if len(node) > 20 else f"{node}"
                else:
                    color = "#45B7D1"  # Azul para palabras clave
                    label = f"{node[:15]}..." if len(node) > 15 else f"{node}"
        else:
            # Red proyectada
            if communities and node in communities:
                color = color_map.get(communities[node], "#97C2FC")
                label = node[:20] + "..." if len(node) > 20 else node
            else:
                color = "#97C2FC"
                label = node[:20] + "..." if len(node) > 20 else node
        
        title = f"{node}<br>Grado: {degree}"
        if communities and node in communities:
            title += f"<br>Comunidad: {communities[node]}"
        
        net.add_node(node, label=label, size=size, color=color, title=title)
    
    # Agregar enlaces
    for source, target, data in G.edges(data=True):
        weight = data.get('weight', 1)
        width = min(0.5 + np.log(weight + 1) * 0.8, 8)
        title = f"Peso: {weight}" if weight > 1 else "Conexi√≥n"
        net.add_edge(source, target, width=width, title=title)
    
    # Guardar y mostrar
    try:
        path = "temp_network.html"
        net.save_graph(path)
        with open(path, 'r', encoding='utf-8') as f:
            html = f.read()
        st.components.v1.html(html, height=600, scrolling=True)
        
        # Limpiar archivo temporal
        if os.path.exists(path):
            os.remove(path)
    except Exception as e:
        st.error(f"Error en visualizaci√≥n: {e}")

def create_metrics_dashboard(metrics):
    """Crea dashboard de m√©tricas de la red"""
    col1, col2, col3, col4 = st.columns(4)
    
    with col1:
        st.metric("Nodos", metrics.get('num_nodes', 0))
        if 'density' in metrics:
            st.metric("Densidad", f"{metrics['density']:.3f}")
    
    with col2:
        st.metric("Enlaces", metrics.get('num_edges', 0))
        if 'avg_clustering' in metrics:
            st.metric("Clustering Promedio", f"{metrics['avg_clustering']:.3f}")
    
    with col3:
        if 'num_components' in metrics:
            st.metric("Componentes", metrics['num_components'])
        if 'num_communities' in metrics:
            st.metric("Comunidades", metrics['num_communities'])
    
    with col4:
        if 'largest_component_size' in metrics:
            st.metric("Componente Mayor", metrics['largest_component_size'])
        if 'modularity' in metrics:
            st.metric("Modularidad", f"{metrics['modularity']:.3f}")

def create_single_centrality_plot(metrics, centrality_type, top_n):
    """Crea un solo gr√°fico de centralidad seg√∫n la selecci√≥n"""
    centrality_map = {
        'Grado': 'degree_centrality',
        'Intermediaci√≥n': 'betweenness_centrality',
        'Cercan√≠a': 'closeness_centrality',
        'Autovector': 'eigenvector_centrality'
    }
    
    metric_key = centrality_map.get(centrality_type)
    
    if not metric_key or metric_key not in metrics or not metrics[metric_key]:
        st.info(f"M√©trica de {centrality_type} no disponible para esta red.")
        return
    
    # Obtener datos y ordenar
    centrality_data = metrics[metric_key]
    sorted_data = sorted(centrality_data.items(), key=lambda x: x[1], reverse=True)[:top_n]
    
    if not sorted_data:
        st.info(f"No hay datos de centralidad de {centrality_type}.")
        return
    
    # Crear gr√°fico
    fig = go.Figure(go.Bar(
        x=[item[1] for item in sorted_data],
        y=[item[0] for item in sorted_data],
        orientation='h',
        marker_color='lightblue'
    ))
    
    fig.update_layout(
        title=f"Top {top_n} - Centralidad de {centrality_type}",
        xaxis_title="Valor de Centralidad",
        yaxis_title="Nodo",
        height=400 + top_n * 15,  # Altura din√°mica
        showlegend=False
    )
    
    st.plotly_chart(fig, use_container_width=True)

def analyze_author_collaboration_patterns(df_filtered, G_coautor, metrics_coautor):
    """An√°lisis espec√≠fico de patrones de colaboraci√≥n entre autores"""
    st.subheader("üîç An√°lisis de Datos: Patrones de Colaboraci√≥n")
    
    # Crear an√°lisis de autores m√°s colaborativos
    autor_stats = {}
    for _, row in df_filtered.iterrows():
        autores = row['autores_list']
        if len(autores) > 1:  # Solo papers colaborativos
            for autor in autores:
                if autor not in autor_stats:
                    autor_stats[autor] = {
                        'colaboraciones': 0,
                        'coautores_unicos': set(),
                        'tematicas': set(),
                        'palabras_clave': [],
                        'instituciones': set()
                    }
                
                autor_stats[autor]['colaboraciones'] += 1
                autor_stats[autor]['coautores_unicos'].update([a for a in autores if a != autor])
                autor_stats[autor]['tematicas'].add(row['tematica'])
                autor_stats[autor]['palabras_clave'].extend(row['palabras_clave_list'])
                autor_stats[autor]['instituciones'].update(row['afiliaciones_list'])
    
    if autor_stats:
        # Convertir a DataFrame para an√°lisis
        autor_df = []
        for autor, stats in autor_stats.items():
            autor_df.append({
                'Autor': autor,
                'Colaboraciones': stats['colaboraciones'],
                'Coautores_Unicos': len(stats['coautores_unicos']),
                'Diversidad_Tematica': len(stats['tematicas']),
                'Diversidad_Institucional': len([inst for inst in stats['instituciones'] if inst]),
                'Palabras_Clave': stats['palabras_clave'],
                'Tematicas': list(stats['tematicas'])
            })
        
        if autor_df:  # Verificar que hay datos
            autor_df = pd.DataFrame(autor_df)
            autor_df = autor_df.sort_values('Colaboraciones', ascending=False)
            
            col1, col2 = st.columns(2)
        else:
            st.warning("No hay suficientes datos de colaboraci√≥n para realizar el an√°lisis.")
            return
        
        with col1:
            st.write("**üèÜ Top 10 Autores M√°s Colaborativos**")
            top_colaborativos = autor_df.head(10)
            
            fig = px.bar(
                top_colaborativos,
                x='Colaboraciones',
                y='Autor',
                orientation='h',
                title="Autores con M√°s Papers Colaborativos"
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.write("**üåê Diversidad vs Colaboraci√≥n**")
            
            fig_scatter = px.scatter(
                autor_df.head(20),
                x='Colaboraciones',
                y='Diversidad_Tematica',
                size='Coautores_Unicos',
                hover_name='Autor',
                title="Colaboraci√≥n vs Diversidad Tem√°tica",
                labels={
                    'Colaboraciones': 'N√∫mero de Colaboraciones',
                    'Diversidad_Tematica': 'N√∫mero de Tem√°ticas',
                    'Coautores_Unicos': 'Coautores √önicos'
                }
            )
            st.plotly_chart(fig_scatter, use_container_width=True)
        
        # An√°lisis detallado de autor seleccionado
        st.subheader("üîç An√°lisis Detallado de Autor")
        autor_seleccionado = st.selectbox(
            "Selecciona un autor para an√°lisis detallado:",
            [''] + list(autor_df['Autor'].values),
            key='autor_colab_analysis'
        )
        
        if autor_seleccionado:
            autor_info = autor_df[autor_df['Autor'] == autor_seleccionado].iloc[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("ü§ù Colaboraciones", autor_info['Colaboraciones'])
            with col2:
                st.metric("üë• Coautores √önicos", autor_info['Coautores_Unicos'])
            with col3:
                st.metric("üìö Tem√°ticas", autor_info['Diversidad_Tematica'])
            with col4:
                st.metric("üèõÔ∏è Instituciones", autor_info['Diversidad_Institucional'])
            
            # Nube de palabras de especializaci√≥n
            if autor_info['Palabras_Clave']:
                st.subheader(f"‚òÅÔ∏è Especializaci√≥n de {autor_seleccionado}")
                
                palabra_freq = Counter(autor_info['Palabras_Clave'])
                if len(palabra_freq) > 0:
                    try:
                        wordcloud = WordCloud(
                            width=800, 
                            height=300, 
                            background_color='white',
                            max_words=30,
                            colormap='viridis'
                        ).generate_from_frequencies(palabra_freq)
                        
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        plt.close()
                    except:
                        st.write("**Principales √°reas de especializaci√≥n:**")
                        for palabra, freq in palabra_freq.most_common(15):
                            st.write(f"‚Ä¢ {palabra} ({freq} veces)")
            
            # Tem√°ticas del autor
            col1, col2 = st.columns(2)
            with col1:
                st.write("**Tem√°ticas de Investigaci√≥n:**")
                for tema in autor_info['Tematicas']:
                    st.write(f"‚Ä¢ {tema}")

def analyze_institutional_collaboration(df_filtered, G_institucional, metrics_inst):
    """An√°lisis espec√≠fico de colaboraci√≥n institucional"""
    st.subheader("üîç An√°lisis de Datos: Colaboraci√≥n Institucional")
    
    # An√°lisis de instituciones
    inst_stats = {}
    for _, row in df_filtered.iterrows():
        instituciones = list(set(row['afiliaciones_list']))  # Eliminar duplicados
        instituciones = [inst for inst in instituciones if inst]  # Filtrar vac√≠os
        
        for inst in instituciones:
            if inst not in inst_stats:
                inst_stats[inst] = {
                    'papers': 0,
                    'autores': set(),
                    'tematicas': set(),
                    'palabras_clave': [],
                    'colaboraciones_inst': set()
                }
            
            inst_stats[inst]['papers'] += 1
            inst_stats[inst]['autores'].update(row['autores_list'])
            inst_stats[inst]['tematicas'].add(row['tematica'])
            inst_stats[inst]['palabras_clave'].extend(row['palabras_clave_list'])
            
            # Colaboraciones con otras instituciones en el mismo paper
            otras_inst = [i for i in instituciones if i != inst]
            inst_stats[inst]['colaboraciones_inst'].update(otras_inst)
    
    if inst_stats:
        # Convertir a DataFrame
        inst_df = []
        for inst, stats in inst_stats.items():
            inst_df.append({
                'Instituci√≥n': inst,
                'Papers': stats['papers'],
                'Autores': len(stats['autores']),
                'Diversidad_Tematica': len(stats['tematicas']),
                'Colaboraciones_Inst': len(stats['colaboraciones_inst']),
                'Palabras_Clave': stats['palabras_clave'],
                'Tematicas': list(stats['tematicas']),
                'Especializaci√≥n': len(stats['tematicas']) / stats['papers'] if stats['papers'] > 0 else 0
            })
        
        if inst_df:  # Verificar que hay datos
            inst_df = pd.DataFrame(inst_df)
            inst_df = inst_df[inst_df['Papers'] >= 2]  # Filtrar instituciones con al menos 2 papers
            
            if len(inst_df) == 0:
                st.warning("No hay instituciones con suficientes papers para realizar el an√°lisis.")
                return
                
            inst_df = inst_df.sort_values('Papers', ascending=False)
            
            col1, col2 = st.columns(2)
        else:
            st.warning("No hay suficientes datos institucionales para realizar el an√°lisis.")
            return
        
        with col1:
            st.write("**üèÜ Top Instituciones por Productividad**")
            top_inst = inst_df.head(10)
            
            fig = px.bar(
                top_inst,
                x='Papers',
                y='Instituci√≥n',
                orientation='h',
                title="Instituciones M√°s Productivas"
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.write("**üéØ Especializaci√≥n vs Diversidad**")
            
            # Crear categor√≠as de especializaci√≥n
            inst_df['Tipo_Especializaci√≥n'] = inst_df['Especializaci√≥n'].apply(
                lambda x: 'Muy Especializada' if x <= 0.3 else 
                         'Especializada' if x <= 0.6 else 
                         'Diversificada'
            )
            
            fig_scatter = px.scatter(
                inst_df.head(15),
                x='Papers',
                y='Diversidad_Tematica',
                size='Autores',
                color='Tipo_Especializaci√≥n',
                hover_name='Instituci√≥n',
                title="Productividad vs Diversidad Tem√°tica",
                labels={
                    'Papers': 'N√∫mero de Papers',
                    'Diversidad_Tematica': 'N√∫mero de Tem√°ticas Diferentes'
                }
            )
            st.plotly_chart(fig_scatter, use_container_width=True)
        
        # An√°lisis de colaboraci√≥n entre instituciones
        st.subheader("ü§ù An√°lisis de Colaboraci√≥n Inter-Institucional")
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Top Instituciones M√°s Colaborativas**")
            top_colab = inst_df.nlargest(10, 'Colaboraciones_Inst')
            
            fig = px.bar(
                top_colab,
                x='Colaboraciones_Inst',
                y='Instituci√≥n',
                orientation='h',
                title="Instituciones con M√°s Colaboraciones"
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # An√°lisis de especializaci√≥n
            especializaci√≥n_counts = inst_df['Tipo_Especializaci√≥n'].value_counts()
            
            fig_pie = px.pie(
                values=especializaci√≥n_counts.values,
                names=especializaci√≥n_counts.index,
                title="Distribuci√≥n de Tipos de Especializaci√≥n"
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        
        # An√°lisis detallado de instituci√≥n
        st.subheader("üîç An√°lisis Detallado de Instituci√≥n")
        inst_seleccionada = st.selectbox(
            "Selecciona una instituci√≥n para an√°lisis detallado:",
            [''] + list(inst_df['Instituci√≥n'].values),
            key='inst_analysis'
        )
        
        if inst_seleccionada:
            inst_info = inst_df[inst_df['Instituci√≥n'] == inst_seleccionada].iloc[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìÑ Papers", inst_info['Papers'])
            with col2:
                st.metric("üë• Autores", inst_info['Autores'])
            with col3:
                st.metric("üìö Tem√°ticas", inst_info['Diversidad_Tematica'])
            with col4:
                st.metric("ü§ù Colaboraciones", inst_info['Colaboraciones_Inst'])
            
            # Nube de palabras de especializaci√≥n institucional
            if inst_info['Palabras_Clave']:
                st.subheader(f"‚òÅÔ∏è √Åreas de Especializaci√≥n - {inst_seleccionada}")
                
                palabra_freq = Counter(inst_info['Palabras_Clave'])
                if len(palabra_freq) > 0:
                    try:
                        wordcloud = WordCloud(
                            width=800, 
                            height=300, 
                            background_color='white',
                            max_words=40,
                            colormap='plasma'
                        ).generate_from_frequencies(palabra_freq)
                        
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        plt.close()
                    except:
                        st.write("**Principales √°reas de investigaci√≥n:**")
                        for palabra, freq in palabra_freq.most_common(20):
                            st.write(f"‚Ä¢ {palabra} ({freq} veces)")
            
            # Informaci√≥n adicional
            col1, col2 = st.columns(2)
            with col1:
                st.write("**Tem√°ticas de Investigaci√≥n:**")
                for tema in inst_info['Tematicas'][:10]:
                    st.write(f"‚Ä¢ {tema}")
            
            with col2:
                tipo_esp = inst_info['Tipo_Especializaci√≥n']
                if tipo_esp == 'Muy Especializada':
                    st.success("üéØ **Instituci√≥n Muy Especializada**: Se enfoca en pocas √°reas espec√≠ficas")
                elif tipo_esp == 'Especializada':
                    st.info("üìä **InstituciÔøΩÔøΩn Especializada**: Tiene un enfoque moderadamente diverso")
                else:
                    st.warning("üåê **Instituci√≥n Diversificada**: Abarca muchas √°reas diferentes")

def analyze_thematic_patterns(df_filtered, G_tematica, metrics_tema):
    """An√°lisis espec√≠fico de patrones tem√°ticos"""
    st.subheader("üîç An√°lisis de Datos: Patrones Tem√°ticos")
    
    # An√°lisis de tem√°ticas
    tema_stats = {}
    for _, row in df_filtered.iterrows():
        tematica = row['tematica']
        if tematica not in tema_stats:
            tema_stats[tematica] = {
                'papers': 0,
                'autores': set(),
                'instituciones': set(),
                'palabras_clave': [],
                'colaboraciones': 0
            }
        
        tema_stats[tematica]['papers'] += 1
        tema_stats[tematica]['autores'].update(row['autores_list'])
        tema_stats[tematica]['instituciones'].update([inst for inst in row['afiliaciones_list'] if inst])
        tema_stats[tematica]['palabras_clave'].extend(row['palabras_clave_list'])
        
        # Contar colaboraciones (papers con m√°s de un autor)
        if len(row['autores_list']) > 1:
            tema_stats[tematica]['colaboraciones'] += 1
    
    if tema_stats:
        # Convertir a DataFrame
        tema_df = []
        for tema, stats in tema_stats.items():
            tema_df.append({
                'Tem√°tica': tema,
                'Papers': stats['papers'],
                'Autores': len(stats['autores']),
                'Instituciones': len(stats['instituciones']),
                'Colaboraciones': stats['colaboraciones'],
                'Tasa_Colaboracion': stats['colaboraciones'] / stats['papers'] if stats['papers'] > 0 else 0,
                'Palabras_Clave': stats['palabras_clave']
            })
        
        if tema_df:  # Verificar que hay datos
            tema_df = pd.DataFrame(tema_df)
            tema_df = tema_df[tema_df['Papers'] >= 2]
            
            if len(tema_df) == 0:
                st.warning("No hay tem√°ticas con suficientes papers para realizar el an√°lisis.")
                return
                
            tema_df = tema_df.sort_values('Papers', ascending=False)
            
            col1, col2 = st.columns(2)
        else:
            st.warning("No hay suficientes datos tem√°ticos para realizar el an√°lisis.")
            return
        
        with col1:
            st.write("**üèÜ Tem√°ticas M√°s Investigadas**")
            top_temas = tema_df.head(10)
            
            fig = px.bar(
                top_temas,
                x='Papers',
                y='Tem√°tica',
                orientation='h',
                title="Tem√°ticas con M√°s Publicaciones"
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.write("**ü§ù Colaboraci√≥n por Tem√°tica**")
            
            fig_scatter = px.scatter(
                tema_df.head(15),
                x='Papers',
                y='Tasa_Colaboracion',
                size='Autores',
                hover_name='Tem√°tica',
                title="Productividad vs Tasa de Colaboraci√≥n",
                labels={
                    'Papers': 'N√∫mero de Papers',
                    'Tasa_Colaboracion': 'Tasa de Colaboraci√≥n',
                    'Autores': 'N√∫mero de Autores'
                }
            )
            st.plotly_chart(fig_scatter, use_container_width=True)
        
        # An√°lisis detallado de tem√°tica
        st.subheader("üîç An√°lisis Detallado de Tem√°tica")
        tema_seleccionada = st.selectbox(
            "Selecciona una tem√°tica para an√°lisis detallado:",
            [''] + list(tema_df['Tem√°tica'].values),
            key='tema_analysis'
        )
        
        if tema_seleccionada:
            tema_info = tema_df[tema_df['Tem√°tica'] == tema_seleccionada].iloc[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìÑ Papers", tema_info['Papers'])
            with col2:
                st.metric("üë• Autores", tema_info['Autores'])
            with col3:
                st.metric("üèõÔ∏è Instituciones", tema_info['Instituciones'])
            with col4:
                st.metric("ü§ù Tasa Colaboraci√≥n", f"{tema_info['Tasa_Colaboracion']:.2%}")
            
            # Nube de palabras de la tem√°tica
            if tema_info['Palabras_Clave']:
                st.subheader(f"‚òÅÔ∏è Palabras Clave - {tema_seleccionada}")
                
                palabra_freq = Counter(tema_info['Palabras_Clave'])
                if len(palabra_freq) > 0:
                    try:
                        wordcloud = WordCloud(
                            width=800, 
                            height=300, 
                            background_color='white',
                            max_words=50,
                            colormap='coolwarm'
                        ).generate_from_frequencies(palabra_freq)
                        
                        fig, ax = plt.subplots(figsize=(10, 4))
                        ax.imshow(wordcloud, interpolation='bilinear')
                        ax.axis('off')
                        st.pyplot(fig)
                        plt.close()
                    except:
                        st.write("**Principales palabras clave:**")
                        for palabra, freq in palabra_freq.most_common(25):
                            st.write(f"‚Ä¢ {palabra} ({freq} veces)")

def analyze_keyword_patterns(df_filtered, G_keywords, metrics_kw):
    """An√°lisis espec√≠fico de patrones de palabras clave"""
    st.subheader("üîç An√°lisis de Datos: Patrones de Palabras Clave")
    
    # An√°lisis de palabras clave m√°s frecuentes
    all_keywords = []
    keyword_authors = {}
    keyword_tematicas = {}
    
    for _, row in df_filtered.iterrows():
        palabras_clave = row['palabras_clave_list']
        autores = row['autores_list']
        tematica = row['tematica']
        
        for kw in palabras_clave:
            if kw:
                all_keywords.append(kw)
                
                if kw not in keyword_authors:
                    keyword_authors[kw] = set()
                    keyword_tematicas[kw] = set()
                
                keyword_authors[kw].update(autores)
                keyword_tematicas[kw].add(tematica)
    
    if all_keywords:
        # An√°lisis de frecuencia de palabras clave
        keyword_freq = Counter(all_keywords)
        
        # Crear DataFrame para an√°lisis
        kw_df = []
        for kw, freq in keyword_freq.items():
            if freq >= 2:  # Solo palabras clave que aparecen al menos 2 veces
                kw_df.append({
                    'Palabra_Clave': kw,
                    'Frecuencia': freq,
                    'Num_Autores': len(keyword_authors[kw]),
                    'Num_Tematicas': len(keyword_tematicas[kw]),
                    'Autores': list(keyword_authors[kw]),
                    'Tematicas': list(keyword_tematicas[kw])
                })
        
        if kw_df:  # Verificar que hay datos
            kw_df = pd.DataFrame(kw_df)
            kw_df = kw_df.sort_values('Frecuencia', ascending=False)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**üèÜ Palabras Clave M√°s Frecuentes**")
                top_keywords = kw_df.head(15)
                
                fig = px.bar(
                    top_keywords,
                    x='Frecuencia',
                    y='Palabra_Clave',
                    orientation='h',
                    title="Palabras Clave M√°s Utilizadas"
                )
                fig.update_layout(height=500)
                st.plotly_chart(fig, use_container_width=True)
            
            with col2:
                st.write("**üåê Diversidad de Palabras Clave**")
                
                fig_scatter = px.scatter(
                    kw_df.head(20),
                    x='Frecuencia',
                    y='Num_Autores',
                    size='Num_Tematicas',
                    hover_name='Palabra_Clave',
                    title="Frecuencia vs N√∫mero de Autores",
                    labels={
                        'Frecuencia': 'Frecuencia de Uso',
                        'Num_Autores': 'N√∫mero de Autores',
                        'Num_Tematicas': 'N√∫mero de Tem√°ticas'
                    }
                )
                st.plotly_chart(fig_scatter, use_container_width=True)
            
            # An√°lisis de especializaci√≥n vs generalizaci√≥n
            st.subheader("üéØ Especializaci√≥n de Palabras Clave")
            
            # Clasificar palabras clave por especializaci√≥n
            kw_df['Especializaci√≥n'] = kw_df['Num_Tematicas'] / kw_df['Frecuencia']
            kw_df['Tipo_Palabra'] = kw_df['Especializaci√≥n'].apply(
                lambda x: 'Muy Espec√≠fica' if x <= 0.3 else 
                         'Espec√≠fica' if x <= 0.6 else 
                         'General'
            )
            
            col1, col2 = st.columns(2)
            
            with col1:
                # Distribuci√≥n de tipos de palabras
                tipo_counts = kw_df['Tipo_Palabra'].value_counts()
                
                fig_pie = px.pie(
                    values=tipo_counts.values,
                    names=tipo_counts.index,
                    title="Distribuci√≥n de Tipos de Palabras Clave"
                )
                st.plotly_chart(fig_pie, use_container_width=True)
            
            with col2:
                st.write("**Palabras Clave M√°s Espec√≠ficas:**")
                especificas = kw_df[kw_df['Tipo_Palabra'] == 'Muy Espec√≠fica'].head(10)
                for _, row in especificas.iterrows():
                    st.write(f"‚Ä¢ **{row['Palabra_Clave']}** ({row['Frecuencia']} usos, {row['Num_Tematicas']} tem√°ticas)")
            
            # An√°lisis detallado de palabra clave
            st.subheader("üîç An√°lisis Detallado de Palabra Clave")
            kw_seleccionada = st.selectbox(
                "Selecciona una palabra clave para an√°lisis detallado:",
                [''] + list(kw_df['Palabra_Clave'].values),
                key='kw_analysis'
            )
            
            if kw_seleccionada:
                kw_info = kw_df[kw_df['Palabra_Clave'] == kw_seleccionada].iloc[0]
                
                col1, col2, col3, col4 = st.columns(4)
                with col1:
                    st.metric("üìä Frecuencia", kw_info['Frecuencia'])
                with col2:
                    st.metric("üë• Autores", kw_info['Num_Autores'])
                with col3:
                    st.metric("üìö Tem√°ticas", kw_info['Num_Tematicas'])
                with col4:
                    tipo = kw_info['Tipo_Palabra']
                    if tipo == 'Muy Espec√≠fica':
                        st.success("üéØ Muy Espec√≠fica")
                    elif tipo == 'Espec√≠fica':
                        st.info("üìä Espec√≠fica")
                    else:
                        st.warning("üåê General")
                
                # Mostrar informaci√≥n detallada
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Principales Autores:**")
                    for autor in kw_info['Autores'][:10]:
                        st.write(f"‚Ä¢ {autor}")
                
                with col2:
                    st.write("**Tem√°ticas Relacionadas:**")
                    for tema in kw_info['Tematicas']:
                        st.write(f"‚Ä¢ {tema}")
        else:
            st.warning("No hay suficientes palabras clave que aparezcan m√∫ltiples veces para realizar el an√°lisis.")

def analyze_institution_thematic_patterns(df_filtered, G_inst_tema, metrics_inst_tema):
    """An√°lisis espec√≠fico de patrones instituci√≥n-tem√°tica"""
    st.subheader("üîç An√°lisis de Datos: Especializaci√≥n Institucional")
    
    # An√°lisis de especializaci√≥n institucional por tem√°tica
    inst_tema_stats = {}
    
    for _, row in df_filtered.iterrows():
        instituciones = [inst for inst in row['afiliaciones_list'] if inst]
        tematica = row['tematica']
        palabras_clave = row['palabras_clave_list']
        autores = row['autores_list']
        
        for inst in set(instituciones):
            if inst not in inst_tema_stats:
                inst_tema_stats[inst] = {}
            
            if tematica not in inst_tema_stats[inst]:
                inst_tema_stats[inst][tematica] = {
                    'papers': 0,
                    'autores': set(),
                    'palabras_clave': []
                }
            
            inst_tema_stats[inst][tematica]['papers'] += 1
            inst_tema_stats[inst][tematica]['autores'].update(autores)
            inst_tema_stats[inst][tematica]['palabras_clave'].extend(palabras_clave)
    
    if inst_tema_stats:
        # Crear an√°lisis de especializaci√≥n
        especializaci√≥n_data = []
        
        for inst, temas in inst_tema_stats.items():
            total_papers = sum(tema_data['papers'] for tema_data in temas.values())
            if total_papers >= 2:  # Solo instituciones con al menos 2 papers
                
                # Encontrar tem√°tica principal
                tema_principal = max(temas.items(), key=lambda x: x[1]['papers'])
                
                especializaci√≥n_data.append({
                    'Instituci√≥n': inst,
                    'Total_Papers': total_papers,
                    'Num_Tematicas': len(temas),
                    'Tema_Principal': tema_principal[0],
                    'Papers_Tema_Principal': tema_principal[1]['papers'],
                    'Porcentaje_Especializaci√≥n': (tema_principal[1]['papers'] / total_papers) * 100,
                    'Autores_Tema_Principal': len(tema_principal[1]['autores']),
                    'Palabras_Clave_Principal': tema_principal[1]['palabras_clave']
                })
        
        if especializaci√≥n_data:  # Verificar que hay datos
            esp_df = pd.DataFrame(especializaci√≥n_data)
            esp_df = esp_df.sort_values('Total_Papers', ascending=False)
            
            # Clasificar instituciones por nivel de especializaci√≥n
            esp_df['Nivel_Especializaci√≥n'] = esp_df['Porcentaje_Especializaci√≥n'].apply(
                lambda x: 'Muy Especializada' if x >= 70 else 
                         'Especializada' if x >= 50 else 
                         'Diversificada'
            )
        else:
            st.warning("No hay suficientes datos institucionales para realizar el an√°lisis de especializaci√≥n.")
            return
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**üèÜ Instituciones M√°s Especializadas**")
            top_esp = esp_df.nlargest(10, 'Porcentaje_Especializaci√≥n')
            
            fig = px.bar(
                top_esp,
                x='Porcentaje_Especializaci√≥n',
                y='Instituci√≥n',
                orientation='h',
                title="Instituciones por Nivel de Especializaci√≥n (%)",
                color='Nivel_Especializaci√≥n'
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            st.write("**üìä Productividad vs Especializaci√≥n**")
            
            fig_scatter = px.scatter(
                esp_df,
                x='Total_Papers',
                y='Porcentaje_Especializaci√≥n',
                size='Num_Tematicas',
                color='Nivel_Especializaci√≥n',
                hover_name='Instituci√≥n',
                title="Productividad vs Especializaci√≥n",
                labels={
                    'Total_Papers': 'N√∫mero Total de Papers',
                    'Porcentaje_Especializaci√≥n': 'Porcentaje de Especializaci√≥n (%)',
                    'Num_Tematicas': 'N√∫mero de Tem√°ticas'
                }
            )
            st.plotly_chart(fig_scatter, use_container_width=True)
        
        # AnÔøΩÔøΩlisis de fortalezas institucionales
        st.subheader("üéØ Fortalezas de Investigaci√≥n por Instituci√≥n")
        
        # Agrupar por tem√°tica principal
        fortalezas = esp_df.groupby('Tema_Principal').agg({
            'Instituci√≥n': 'count',
            'Papers_Tema_Principal': 'sum',
            'Autores_Tema_Principal': 'sum'
        }).reset_index()
        
        fortalezas.columns = ['Tem√°tica', 'Num_Instituciones', 'Total_Papers', 'Total_Autores']
        fortalezas = fortalezas.sort_values('Total_Papers', ascending=False)
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.write("**Tem√°ticas con M√°s Instituciones Especializadas:**")
            top_fortalezas = fortalezas.head(10)
            
            fig = px.bar(
                top_fortalezas,
                x='Num_Instituciones',
                y='Tem√°tica',
                orientation='h',
                title="Tem√°ticas por N√∫mero de Instituciones"
            )
            fig.update_layout(height=400)
            st.plotly_chart(fig, use_container_width=True)
        
        with col2:
            # Distribuci√≥n de niveles de especializaci√≥n
            nivel_counts = esp_df['Nivel_Especializaci√≥n'].value_counts()
            
            fig_pie = px.pie(
                values=nivel_counts.values,
                names=nivel_counts.index,
                title="Distribuci√≥n de Niveles de Especializaci√≥n"
            )
            st.plotly_chart(fig_pie, use_container_width=True)
        
        # An√°lisis detallado de instituci√≥n
        st.subheader("üîç An√°lisis Detallado de Especializaci√≥n Institucional")
        inst_seleccionada = st.selectbox(
            "Selecciona una instituci√≥n para an√°lisis detallado:",
            [''] + list(esp_df['Instituci√≥n'].values),
            key='inst_tema_analysis'
        )
        
        if inst_seleccionada:
            inst_info = esp_df[esp_df['Instituci√≥n'] == inst_seleccionada].iloc[0]
            
            col1, col2, col3, col4 = st.columns(4)
            with col1:
                st.metric("üìÑ Total Papers", inst_info['Total_Papers'])
            with col2:
                st.metric("üìö Tem√°ticas", inst_info['Num_Tematicas'])
            with col3:
                st.metric("üéØ Especializaci√≥n", f"{inst_info['Porcentaje_Especializaci√≥n']:.1f}%")
            with col4:
                nivel = inst_info['Nivel_Especializaci√≥n']
                if nivel == 'Muy Especializada':
                    st.success("üéØ Muy Especializada")
                elif nivel == 'Especializada':
                    st.info("ÔøΩÔøΩÔøΩ Especializada")
                else:
                    st.warning("üåê Diversificada")
            
            # Informaci√≥n de la tem√°tica principal
            st.subheader(f"üèÜ Fortaleza Principal: {inst_info['Tema_Principal']}")
            
            col1, col2 = st.columns(2)
            with col1:
                st.metric("üìÑ Papers en Tema Principal", inst_info['Papers_Tema_Principal'])
                st.metric("üë• Autores en Tema Principal", inst_info['Autores_Tema_Principal'])
            
            with col2:
                # Nube de palabras de la especializaci√≥n
                if inst_info['Palabras_Clave_Principal']:
                    palabra_freq = Counter(inst_info['Palabras_Clave_Principal'])
                    if len(palabra_freq) > 0:
                        try:
                            wordcloud = WordCloud(
                                width=600, 
                                height=300, 
                                background_color='white',
                                max_words=30,
                                colormap='viridis'
                            ).generate_from_frequencies(palabra_freq)
                            
                            fig, ax = plt.subplots(figsize=(8, 4))
                            ax.imshow(wordcloud, interpolation='bilinear')
                            ax.axis('off')
                            ax.set_title(f"Especializaci√≥n en {inst_info['Tema_Principal']}")
                            st.pyplot(fig)
                            plt.close()
                        except:
                            st.write("**Palabras clave principales:**")
                            for palabra, freq in palabra_freq.most_common(15):
                                st.write(f"‚Ä¢ {palabra} ({freq})")
            
            # Mostrar todas las tem√°ticas de la instituci√≥n
            if inst_seleccionada in inst_tema_stats:
                st.subheader("üìä Distribuci√≥n Completa de Tem√°ticas")
                
                temas_inst = []
                for tema, data in inst_tema_stats[inst_seleccionada].items():
                    temas_inst.append({
                        'Tem√°tica': tema,
                        'Papers': data['papers'],
                        'Autores': len(data['autores']),
                        'Porcentaje': (data['papers'] / inst_info['Total_Papers']) * 100
                    })
                
                temas_df = pd.DataFrame(temas_inst)
                temas_df = temas_df.sort_values('Papers', ascending=False)
                
                fig = px.bar(
                    temas_df,
                    x='Papers',
                    y='Tem√°tica',
                    orientation='h',
                    title=f"Distribuci√≥n de Papers por Tem√°tica - {inst_seleccionada}"
                )
                fig.update_layout(height=300 + len(temas_df) * 20)
                st.plotly_chart(fig, use_container_width=True)

##INTERFAZ PRINCIPAL
with st.spinner("üîÑ Cargando datos de papers cubanos..."):
    df, stats = load_and_analyze_data()

if df is None:
    st.stop()

filters = create_sidebar_filters(df)

df_filtered = apply_filters(df, filters)

tabs = st.tabs([
    "üìä General", 
    "ü§ù Red de Coautor√≠a", 
    "üè¢ Red Institucional", 
    "üìö Red Tem√°tica",
    "üîë Red de Palabras Clave",
    "üèõÔ∏è Red Instituci√≥n-Tem√°tica"
])

with tabs[0]:
    st.subheader("üìä Estad√≠sticas Generales")

    col1, col2, col3, col4 = st.columns(4)
    with col1:
        st.metric("Total Papers", f"{stats['total_papers']:,}")
    with col2:
        st.metric("Total Autores", f"{stats['total_autores']:,}")
    with col3:
        st.metric("Total Instituciones", f"{stats['total_instituciones']:,}")
    with col4:
        st.metric("Total Palabras Clave", f"{stats['total_palabras_clave']:,}")
    
    st.subheader("üõ†Ô∏è Caracter√≠sticas de la Herramienta")

    st.markdown("""
    #### **Funcionalidades Principales:**
    - **Filtros**: Filtra por tem√°tica e instituci√≥n para crear subgrafos espec√≠ficos
    - **M√∫ltiples Tipos de Redes**: Coautor√≠a, institucional, tem√°tica, palabras clave, instituci√≥n-tem√°tica.
    - **Visualizaciones Interactivas**: Redes interactivas con informaci√≥n detallada al pasar el mouse
    - **M√©tricas de Centralidad**: Identifica autores e instituciones m√°s importantes
    - **Detecci√≥n de Comunidades**: Encuentra grupos de investigaci√≥n colaborativa
    """)
        
    st.info("üí°M√©tricas")
    col1, col2, col3 = st.columns(3)
    with col1:
        st.markdown("""
        ##### **M√©tricas de Centralidad:**
        - **Centralidad de Grado**:  Qu√© tan conectado est√° un nodo. √ötil para identificar autores con muchas colaboraciones.
        - **Centralidad de Intermediaci√≥n (Betweenness)**: Qu√© tan frecuentemente un nodo est√° en el camino m√°s corto entre otros dos. Revela autores puente.
        - **Centralidad de Cercan√≠a (Closeness)**: Qu√© tan r√°pido puede acceder un nodo al resto de la red. Ideal para detectar nodos con buena difusi√≥n.
        - **Centralidad de Eigenvector:**: Eval√∫a la influencia de un nodo tomando en cuenta la importancia de sus vecinos
        """)
    
    with col2:
        st.markdown("""
        ##### **M√©tricas de agrupamiento y cohesi√≥n:**
        - **Coeficiente de agrupamiento (Clustering coefficient)**: Mide qu√© tan conectados est√°n los vecinos de un nodo entre s√≠. Perfecto para detectar comunidades cient√≠ficas.
        - **Modularidad**: Ayuda a identificar comunidades dentro de la red (ej. grupos de autores que suelen colaborar entre s√≠).
        """)
        
    with col3:
        st.markdown("""
        ##### **M√©tricas de alcance e influencia:**
        - **Densidad**: Cu√°ntos enlaces existen en relaci√≥n a todos los posibles. Una red muy densa podr√≠a indicar un campo de investigaci√≥n muy interconectado.
        """)
        
with tabs[1]:
    st.header("ü§ù Red de Coautor√≠a")
    st.markdown("""
    - Identifica patrones de colaboraci√≥n entre investigadores cubanos.
    - Revela qui√©nes son los autores m√°s conectados e influyentes.
    - Muestra comunidades de investigaci√≥n que trabajan juntas frecuentemente.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        with st.spinner("Construyendo red de coautor√≠a..."):
            G_coautor = build_filtered_coauthorship_network(
                df_filtered, 
                filters['min_colaboraciones'], 
                filters['max_nodos']
            )
        
        if G_coautor.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_coautor = calculate_network_metrics(G_coautor, "coautoria")
            
            create_network_visualization(G_coautor, metrics_coautor, "Red de Coautor√≠a")
            
            if 'degree_centrality' in metrics_coautor:
                st.subheader("An√°lisis de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona la m√©trica de centralidad:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a', 'Eigenvector'],
                        key='centrality_coautor'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Top N a mostrar:",
                        [5, 10, 15, 20, 25],
                        index=1,
                        key='top_n_coautor'
                    )
                
                create_single_centrality_plot(metrics_coautor, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de colaboraci√≥n de autores
            analyze_author_collaboration_patterns(df_filtered, G_coautor, metrics_coautor)
        else:
            st.warning("No hay suficientes datos para construir la red de coautor√≠a con los filtros actuales.")

with tabs[2]:
    st.header("üè¢ Red de Colaboraci√≥n Institucional")
    st.markdown("""
    - Analiza colaboraciones entre instituciones cubanas.
    - Identifica instituciones que act√∫an como puentes entre diferentes grupos.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        min_shared = st.slider(
            "Autores compartidos m√≠nimos entre instituciones:", 
            1, 5, 1, 
            key='inst_shared',
            help="Dos instituciones se conectan si comparten al menos este n√∫mero de autores"
        )
        
        with st.spinner("Construyendo red institucional..."):
            G_institucional = build_filtered_institution_network(
                df_filtered, 
                min_shared, 
                filters['max_nodos']//2
            )
        
        if G_institucional.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_inst = calculate_network_metrics(G_institucional, "institucional")
            
            create_network_visualization(G_institucional, metrics_inst, "Red Institucional")
            
            if 'degree_centrality' in metrics_inst:
                st.subheader("An√°lisis de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona m√©trica de centralidad:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a', 'Eigenvector'],
                        key='centrality_inst'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Top N a mostrar:",
                        [5, 10, 15, 20],
                        index=1,
                        key='top_n_inst'
                    )
                
                create_single_centrality_plot(metrics_inst, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de colaboraci√≥n institucional
            analyze_institutional_collaboration(df_filtered, G_institucional, metrics_inst)
        else:
            st.warning("No hay suficientes datos para construir la red institucional con los filtros actuales.")

with tabs[3]:
    st.header("üìö Red Tem√°tica (Autor-Tem√°tica)")
    st.markdown("""
    - Conecta autores con las tem√°ticas que investigan.
    - Identifica especialistas en cada √°rea.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        with st.spinner("Construyendo red tem√°tica..."):
            G_tematica = build_thematic_network(df_filtered, filters['max_nodos'])
        
        if G_tematica.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_tema = calculate_network_metrics(G_tematica, "tematica")
            
            st.info("üî¥ Nodos rojos = Autores | üü¢ Nodos verdes = Tem√°ticas")
            create_network_visualization(G_tematica, metrics_tema, "Red Tem√°tica")
            
            # M√©tricas de centralidad para red bipartita
            if 'degree_centrality' in metrics_tema:
                st.subheader("An√°lisis de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona m√©trica de centralidad:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a'],
                        key='centrality_tema'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Top N a mostrar:",
                        [5, 10, 15, 20],
                        index=1,
                        key='top_n_tema'
                    )
                
                create_single_centrality_plot(metrics_tema, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de patrones tem√°ticos
            analyze_thematic_patterns(df_filtered, G_tematica, metrics_tema)
        else:
            st.warning("No hay suficientes datos para construir la red tem√°tica con los filtros actuales.")

with tabs[4]:
    st.header("üîë Red de Palabras Clave (Autor-Palabra Clave)")
    st.markdown("""
    - An√°lisis de los intereses de investigaci√≥n.
    - Conecta investigadores con intereses espec√≠ficos similares.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        with st.spinner("Construyendo red de palabras clave..."):
            G_keywords = build_keyword_network(df_filtered, filters['max_nodos'])
        
        if G_keywords.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_kw = calculate_network_metrics(G_keywords, "keywords")
            
            st.info("üî¥ Nodos rojos = Autores | üîµ Nodos azules = Palabras Clave")
            create_network_visualization(G_keywords, metrics_kw, "Red de Palabras Clave")
            
            if 'degree_centrality' in metrics_kw:
                st.subheader("An√°lisis de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona m√©trica de centralidad:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a'],
                        key='centrality_kw'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Top N a mostrar:",
                        [5, 10, 15, 20, 25],
                        index=2,
                        key='top_n_kw'
                    )
                
                create_single_centrality_plot(metrics_kw, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de patrones de palabras clave
            analyze_keyword_patterns(df_filtered, G_keywords, metrics_kw)
        else:
            st.warning("No hay suficientes datos para construir la red de palabras clave con los filtros actuales.")

with tabs[5]:
    st.header("üèõÔ∏è Red Instituci√≥n-Tem√°tica")
    st.markdown("""
    - Identifica fortalezas de investigaci√≥n, conecta instituciones con las tem√°ticas que publican.
    - Revela qu√© instituciones se especializan en qu√© √°reas.
    - Muestra instituciones con mayor diversidad de investigaci√≥n.
    - Identifica instituciones que comparten intereses de investigaci√≥n.
    """)
    
    if len(df_filtered) == 0:
        st.warning("‚ö†Ô∏è No hay papers que coincidan con los filtros seleccionados.")
    else:
        with st.spinner("Construyendo red Instituci√≥n-Tem√°tica..."):
            G_inst_tema = build_institution_thematic_network(df_filtered, filters['max_nodos'])
        
        if G_inst_tema.number_of_nodes() > 0:
            # Calcular m√©tricas espec√≠ficas para esta red
            metrics_inst_tema = calculate_network_metrics(G_inst_tema, "institucion_tematica")
            
            st.info("üîµ Nodos azules = Instituciones | üü¢ Nodos verdes = Tem√°ticas")
            create_network_visualization(G_inst_tema, metrics_inst_tema, "Red Instituci√≥n-Tem√°tica")
            
            # Crear an√°lisis de especializaci√≥n por instituci√≥n
            instituciones = [n for n, d in G_inst_tema.nodes(data=True) if d.get('bipartite') == 0]
            tematicas = [n for n, d in G_inst_tema.nodes(data=True) if d.get('bipartite') == 1]
            
            if instituciones and tematicas:
                # An√°lisis de diversidad tem√°tica por instituci√≥n
                inst_diversidad = {}
                inst_fortalezas = {}
                
                for inst in instituciones:
                    # Obtener tem√°ticas conectadas y sus pesos
                    temas_conectadas = []
                    pesos_temas = []
                    
                    for tema in G_inst_tema.neighbors(inst):
                        peso = G_inst_tema[inst][tema].get('weight', 1)
                        temas_conectadas.append(tema)
                        pesos_temas.append(peso)
                    
                    inst_diversidad[inst] = len(temas_conectadas)
                    
                    # Identificar fortaleza principal (tem√°tica con mayor peso)
                    if pesos_temas:
                        idx_max = pesos_temas.index(max(pesos_temas))
                        inst_fortalezas[inst] = {
                            'tema_principal': temas_conectadas[idx_max],
                            'num_papers': max(pesos_temas),
                            'diversidad': len(temas_conectadas),
                            'total_papers': sum(pesos_temas)
                        }
                
            # M√©tricas de centralidad
            if 'degree_centrality' in metrics_inst_tema:
                st.subheader("An√°lisis de Centralidad")
                
                col1, col2 = st.columns(2)
                with col1:
                    centrality_type = st.selectbox(
                        "Selecciona m√©trica de centralidad:",
                        ['Grado', 'Intermediaci√≥n', 'Cercan√≠a'],
                        key='centrality_inst_tema'
                    )
                with col2:
                    top_n = st.selectbox(
                        "Top N a mostrar:",
                        [5, 10, 15, 20],
                        index=1,
                        key='top_n_inst_tema'
                    )

                create_single_centrality_plot(metrics_inst_tema, centrality_type, top_n)
            
            # An√°lisis espec√≠fico de especializaci√≥n institucional
            analyze_institution_thematic_patterns(df_filtered, G_inst_tema, metrics_inst_tema)
        else:
            st.warning("No hay suficientes datos para construir la red Instituci√≥n-Tem√°tica con los filtros actuales.")
    st.markdown("""
    Esta secci√≥n te permite explorar los datos de manera interactiva para responder preguntas espec√≠ficas sobre:
    - **Autores**: ¬øQui√©nes son los m√°s productivos? ¬øEn qu√© instituciones trabajan?
    - **Instituciones**: ¬øCu√°les son las m√°s activas? ¬øEn qu√© √°reas se especializan?
    - **Colaboraciones**: ¬øQu√© patrones de colaboraci√≥n existen?
    - **Tem√°ticas**: ¬øCu√°les son las √°reas de investigaci√≥n m√°s populares?
    """)
    
    # Selector de tipo de an√°lisis
    analysis_type = st.selectbox(
        "¬øQu√© quieres explorar?",
        [
            "üßë‚Äçüî¨ An√°lisis de Autores",
            "üèõÔ∏è An√°lisis de Instituciones", 
            "An√°lisis de Colaboraciones",
            "üìö An√°lisis de Tem√°ticas",
            "üîç B√∫squeda Espec√≠fica"
        ]
    )
    
    if analysis_type == "üßë‚Äçüî¨ An√°lisis de Autores":
        st.subheader("An√°lisis Detallado de Autores")
        
        # Crear an√°lisis de autores
        autor_stats = []
        for _, row in df_filtered.iterrows():
            autores = row['autores_list']
            instituciones = row['afiliaciones_list']
            tematica = row['tematica']
            palabras_clave = row['palabras_clave_list']
            
            for autor in autores:
                autor_stats.append({
                    'autor': autor,
                    'instituciones': instituciones,
                    'tematica': tematica,
                    'palabras_clave': palabras_clave,
                    'titulo': row['titulo']
                })
        
        if autor_stats:
            autor_df = pd.DataFrame(autor_stats)
            
            # Estad√≠sticas por autor
            autor_summary = autor_df.groupby('autor').agg({
                'titulo': 'count',
                'tematica': lambda x: list(set(x)),
                'instituciones': lambda x: list(set([inst for sublist in x for inst in sublist if inst])),
            }).reset_index()
            
            autor_summary.columns = ['Autor', 'Num_Papers', 'Tem√°ticas', 'Instituciones']
            autor_summary = autor_summary.sort_values('Num_Papers', ascending=False)
            
            # Top autores m√°s productivos
            st.subheader("üèÜ Top Autores M√°s Productivos")
            top_n_autores = st.slider("N√∫mero de autores a mostrar:", 5, 50, 20)
            
            top_autores = autor_summary.head(top_n_autores)
            
            # Gr√°fico de barras
            fig = px.bar(
                top_autores, 
                x='Num_Papers', 
                y='Autor',
                orientation='h',
                title=f"Top {top_n_autores} Autores por N√∫mero de Publicaciones",
                labels={'Num_Papers': 'N√∫mero de Papers', 'Autor': 'Autor'}
            )
            fig.update_layout(height=600)
            st.plotly_chart(fig, use_container_width=True)
            
            # B√∫squeda de autor espec√≠fico
            st.subheader("üîç Informaci√≥n Detallada de Autor")
            autor_seleccionado = st.selectbox(
                "Selecciona un autor para ver detalles:",
                [''] + list(autor_summary['Autor'].values)
            )
            
            if autor_seleccionado:
                autor_info = autor_summary[autor_summary['Autor'] == autor_seleccionado].iloc[0]
                
                col1, col2 = st.columns(2)
                
                with col1:
                    st.metric("üìÑ N√∫mero de Papers", autor_info['Num_Papers'])
                    st.metric("üèõÔ∏è N√∫mero de Instituciones", len(autor_info['Instituciones']))
                    
                    st.write("**Instituciones:**")
                    for inst in autor_info['Instituciones']:
                        st.write(f"‚Ä¢ {inst}")
                
                with col2:
                    st.metric("üìö N√∫mero de Tem√°ticas", len(autor_info['Tem√°ticas']))
                    
                    st.write("**Tem√°ticas de Investigaci√≥n:**")
                    for tema in autor_info['Tem√°ticas']:
                        st.write(f"‚Ä¢ {tema}")
                
                # Papers del autor
                papers_autor = autor_df[autor_df['autor'] == autor_seleccionado]
                st.subheader(f"üìÑ Papers de {autor_seleccionado}")
                
                for i, (_, paper) in enumerate(papers_autor.iterrows(), 1):
                    with st.expander(f"Paper {i}: {paper['titulo'][:100]}..."):
                        st.write(f"**T√≠tulo:** {paper['titulo']}")
                        st.write(f"**Tem√°tica:** {paper['tematica']}")
                        if paper['palabras_clave']:
                            st.write(f"**Palabras Clave:** {', '.join(paper['palabras_clave'])}")
    
    elif analysis_type == "üèõÔ∏è An√°lisis de Instituciones":
        st.subheader("An√°lisis Detallado de Instituciones")
        
        # Crear an√°lisis de instituciones
        inst_stats = []
        for _, row in df_filtered.iterrows():
            autores = row['autores_list']
            instituciones = row['afiliaciones_list']
            tematica = row['tematica']
            
            for inst in set(instituciones):
                if inst:
                    inst_stats.append({
                        'institucion': inst,
                        'autores': autores,
                        'tematica': tematica,
                        'titulo': row['titulo']
                    })
        
        if inst_stats:
            inst_df = pd.DataFrame(inst_stats)
            
            # Estad√≠sticas por instituci√≥n
            inst_summary = inst_df.groupby('institucion').agg({
                'titulo': 'count',
                'tematica': lambda x: list(set(x)),
                'autores': lambda x: list(set([autor for sublist in x for autor in sublist])),
            }).reset_index()
            
            inst_summary.columns = ['Instituci√≥n', 'Num_Papers', 'Tem√°ticas', 'Autores']
            inst_summary['Num_Autores'] = inst_summary['Autores'].apply(len)
            inst_summary['Num_Tem√°ticas'] = inst_summary['Tem√°ticas'].apply(len)
            inst_summary = inst_summary.sort_values('Num_Papers', ascending=False)
            
            # Top instituciones
            st.subheader("üèÜ Top Instituciones M√°s Productivas")
            top_n_inst = st.slider("N√∫mero de instituciones a mostrar:", 5, 30, 15)
            
            top_instituciones = inst_summary.head(top_n_inst)
            
            # Gr√°fico de barras
            fig = px.bar(
                top_instituciones, 
                x='Num_Papers', 
                y='Instituci√≥n',
                orientation='h',
                title=f"Top {top_n_inst} Instituciones por N√∫mero de Publicaciones"
            )
            fig.update_layout(height=600)
            st.plotly_chart(fig, use_container_width=True)
            
            # An√°lisis de diversidad institucional
            st.subheader("üìä Diversidad de Investigaci√≥n por Instituci√≥n")
            
            # Gr√°fico de dispersi√≥n: Papers vs Diversidad Tem√°tica
            fig_scatter = px.scatter(
                inst_summary.head(20),
                x='Num_Papers',
                y='Num_Tem√°ticas',
                size='Num_Autores',
                hover_name='Instituci√≥n',
                title="Productividad vs Diversidad Tem√°tica",
                labels={
                    'Num_Papers': 'N√∫mero de Papers',
                    'Num_Tem√°ticas': 'N√∫mero de Tem√°ticas',
                    'Num_Autores': 'N√∫mero de Autores'
                }
            )
            st.plotly_chart(fig_scatter, use_container_width=True)
            
            # Informaci√≥n detallada de instituci√≥n
            st.subheader("üîç Informaci√≥n Detallada de Instituci√≥n")
            inst_seleccionada = st.selectbox(
                "Selecciona una instituci√≥n:",
                [''] + list(inst_summary['Instituci√≥n'].values)
            )
            
            if inst_seleccionada:
                inst_info = inst_summary[inst_summary['Instituci√≥n'] == inst_seleccionada].iloc[0]
                
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    st.metric("üìÑ Papers", inst_info['Num_Papers'])
                with col2:
                    st.metric("üë• Autores", inst_info['Num_Autores'])
                with col3:
                    st.metric("üìö Tem√°ticas", inst_info['Num_Tem√°ticas'])
                
                # Mostrar tem√°ticas y autores principales
                col1, col2 = st.columns(2)
                
                with col1:
                    st.write("**Principales Tem√°ticas:**")
                    for tema in inst_info['Tem√°ticas'][:10]:
                        st.write(f"‚Ä¢ {tema}")
                
                with col2:
                    st.write("**Principales Autores:**")
                    for autor in inst_info['Autores'][:10]:
                        st.write(f"‚Ä¢ {autor}")
    
    elif analysis_type == "ü§ù An√°lisis de Colaboraciones":
        st.subheader("An√°lisis de Patrones de Colaboraci√≥n")
        
        # An√°lisis de colaboraciones entre autores
        colaboraciones = []
        for _, row in df_filtered.iterrows():
            autores = row['autores_list']
            if len(autores) > 1:
                for i in range(len(autores)):
                    for j in range(i+1, len(autores)):
                        colaboraciones.append({
                            'autor1': autores[i],
                            'autor2': autores[j],
                            'tematica': row['tematica'],
                            'titulo': row['titulo']
                        })
        
        if colaboraciones:
            colab_df = pd.DataFrame(colaboraciones)
            
            # Top colaboraciones
            colab_counts = colab_df.groupby(['autor1', 'autor2']).size().reset_index(name='num_colaboraciones')
            colab_counts = colab_counts.sort_values('num_colaboraciones', ascending=False)
            
            st.subheader("üèÜ Top Colaboraciones Entre Autores")
            top_n_colab = st.slider("N√∫mero de colaboraciones a mostrar:", 5, 30, 15)
            
            top_colaboraciones = colab_counts.head(top_n_colab)
            top_colaboraciones['colaboracion'] = top_colaboraciones['autor1'] + ' ‚Üî ' + top_colaboraciones['autor2']
            
            fig = px.bar(
                top_colaboraciones,
                x='num_colaboraciones',
                y='colaboracion',
                orientation='h',
                title=f"Top {top_n_colab} Colaboraciones M√°s Frecuentes"
            )
            fig.update_layout(height=600)
            st.plotly_chart(fig, use_container_width=True)
            
            # An√°lisis de colaboraci√≥n por tem√°tica
            st.subheader("üìö Colaboraciones por Tem√°tica")
            
            tematica_colab = colab_df.groupby('tematica').size().reset_index(name='num_colaboraciones')
            tematica_colab = tematica_colab.sort_values('num_colaboraciones', ascending=False).head(15)
            
            fig_tema = px.bar(
                tematica_colab,
                x='num_colaboraciones',
                y='tematica',
                orientation='h',
                title="Colaboraciones por Tem√°tica de Investigaci√≥n"
            )
            fig_tema.update_layout(height=500)
            st.plotly_chart(fig_tema, use_container_width=True)
    
    elif analysis_type == "üìö An√°lisis de Tem√°ticas":
        st.subheader("An√°lisis Detallado de Tem√°ticas de Investigaci√≥n")
        
        # An√°lisis de tem√°ticas
        tematica_stats = df_filtered.groupby('tematica').agg({
            'titulo': 'count',
            'autores_list': lambda x: list(set([autor for sublist in x for autor in sublist])),
            'afiliaciones_list': lambda x: list(set([inst for sublist in x for inst in sublist if inst])),
            'palabras_clave_list': lambda x: [kw for sublist in x for kw in sublist if kw]
        }).reset_index()
        
        tematica_stats.columns = ['Tem√°tica', 'Num_Papers', 'Autores', 'Instituciones', 'Palabras_Clave']
        tematica_stats['Num_Autores'] = tematica_stats['Autores'].apply(len)
        tematica_stats['Num_Instituciones'] = tematica_stats['Instituciones'].apply(len)
        tematica_stats = tematica_stats.sort_values('Num_Papers', ascending=False)
        
        # Top tem√°ticas
        st.subheader("üèÜ Tem√°ticas M√°s Investigadas")
        top_n_temas = st.slider("N√∫mero de tem√°ticas a mostrar:", 5, 25, 15)
        
        top_tematicas = tematica_stats.head(top_n_temas)
        
        fig = px.bar(
            top_tematicas,
            x='Num_Papers',
            y='Tem√°tica',
            orientation='h',
            title=f"Top {top_n_temas} Tem√°ticas por N√∫mero de Papers"
        )
        fig.update_layout(height=600)
        st.plotly_chart(fig, use_container_width=True)
        
        # An√°lisis detallado de tem√°tica
        st.subheader("üîç An√°lisis Detallado de Tem√°tica")
        tema_seleccionada = st.selectbox(
            "Selecciona una tem√°tica:",
            [''] + list(tematica_stats['Tem√°tica'].values)
        )
        
        if tema_seleccionada:
            tema_info = tematica_stats[tematica_stats['Tem√°tica'] == tema_seleccionada].iloc[0]
            
            col1, col2, col3 = st.columns(3)
            
            with col1:
                st.metric("üìÑ Papers", tema_info['Num_Papers'])
            with col2:
                st.metric("üë• Autores", tema_info['Num_Autores'])
            with col3:
                st.metric("üèõÔ∏è Instituciones", tema_info['Num_Instituciones'])
            
            # Mostrar palabras clave principales
            if tema_info['Palabras_Clave']:
                st.subheader("üîë Principales Palabras Clave")
                
                # Contar frecuencia de palabras clave
                palabra_freq = Counter(tema_info['Palabras_Clave'])
                
                if len(palabra_freq) > 0:
                    st.write("**Palabras Clave M√°s Frecuentes:**")
                    for palabra, freq in palabra_freq.most_common(20):
                        st.write(f"‚Ä¢ {palabra} ({freq})")
            
            # Top autores e instituciones en esta tem√°tica
            col1, col2 = st.columns(2)
            
            with col1:
                st.write("**Top Autores:**")
                for autor in tema_info['Autores'][:10]:
                    st.write(f"‚Ä¢ {autor}")
            
            with col2:
                st.write("**Top Instituciones:**")
                for inst in tema_info['Instituciones'][:10]:
                    st.write(f"‚Ä¢ {inst}")
    
    elif analysis_type == "üîç B√∫squeda Espec√≠fica":
        st.subheader("B√∫squeda Espec√≠fica en la Base de Datos")
        
        # Opciones de b√∫squeda
        search_type = st.selectbox(
            "¬øQu√© quieres buscar?",
            [
                "Buscar por Autor",
                "Buscar por Instituci√≥n", 
                "Buscar por Palabra Clave",
                "Buscar por T√≠tulo"
            ]
        )
        
        search_term = st.text_input("Introduce el t√©rmino de b√∫squeda:")
        
        if search_term:
            if search_type == "Buscar por Autor":
                # Buscar papers que contengan el autor
                mask = df_filtered['autores_normalizados'].str.contains(search_term, case=False, na=False)
                resultados = df_filtered[mask]
                
            elif search_type == "Buscar por Instituci√≥n":
                mask = df_filtered['afiliaciones_normalizadas'].str.contains(search_term, case=False, na=False)
                resultados = df_filtered[mask]
                
            elif search_type == "Buscar por Palabra Clave":
                mask = df_filtered['palabras_clave'].str.contains(search_term, case=False, na=False)
                resultados = df_filtered[mask]
                
            elif search_type == "Buscar por T√≠tulo":
                mask = df_filtered['titulo'].str.contains(search_term, case=False, na=False)
                resultados = df_filtered[mask]
            
            if len(resultados) > 0:
                st.success(f"Se encontraron {len(resultados)} resultados para '{search_term}'")
                
                # Mostrar resultados
                for i, (_, row) in enumerate(resultados.iterrows(), 1):
                    with st.expander(f"Resultado {i}: {row['titulo'][:80]}..."):
                        st.write(f"**T√≠tulo:** {row['titulo']}")
                        st.write(f"**Autores:** {row['autores_normalizados']}")
                        st.write(f"**Instituciones:** {row['afiliaciones_normalizadas']}")
                        st.write(f"**Tem√°tica:** {row['tematica']}")
                        if row['palabras_clave']:
                            st.write(f"**Palabras Clave:** {row['palabras_clave']}")
            else:
                st.warning(f"No se encontraron resultados para '{search_term}'")
